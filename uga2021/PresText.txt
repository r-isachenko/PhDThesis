Slide 1:
Hello, everyone! My name is Roman Isachenko. I am PhD student at MIPT. The topic of my today talk is Dimensionality Reduction for Signal Decoding.

Slide 2:
In this study we are considering the problem of selection of optimal decoding model.
The decoding process essentially is a process restoring the dependencies between two heterogeneous data sets.
The challenge is that we consider the case where target variable is a vector.
The spaces of the input signals and target signals have excessive dimensionality.
Dimensionality reduction methods that do not take into account dependencies in the target space are not adequate.
We propose to reduce the dimensionality of the input and target spaces by projecting signals into hidden spaces of significantly smaller dimensionality.
Linear and nonlinear methods for building concordant models are proposed.

Slide 3:
Our goal is to build a decoding model f, which takes the input signal x and predicts the target signal y.
The slide shows an example of the input and target time series for the task of restoring the hand trajectory based on the signals of the electrocorticogram.
The input and target signals have a different nature and have a high dimensionality.
A point in the x-space is a segment of the input signal from the space of dimension n.
A point in the target space y is a segment of the target signal from the space of dimensionality r.
The complexity of the problem lies in the fact that at each moment of time t, the forecasting decoding model have to predict a vector consisting of successive components of the target signal.
To construct an adequate model f, a procedure for dependencies concordance  in a hidden space is proposed.
In order to do this, the input and target signals are projected into the hidden space, which has a smaller dimensionality compared to the dimensionality of the original data description.
The operators P and Q restore the input and target variables from the hidden space projections.
At the same time, it is proposed to concord the projections in such a way that the covariance between the projections is maximal.

Slide 4:
Let consider the case of a linear signal decoding model.
The input and target variables form the X and Y matrices.
The model parameters Theta are a matrix.
We will use the simple quadratic l2 loss function between the target variable and the forecast for this decoding model.
If there is a strong multicorrelation between the columns of the matrix of the frequency domain of the input signals, the decoding model turns out to be unstable.
In this moment we propose a method of projection data descriptions into a hidden space.
The original matrices X and Y are projected into hidden spaces.
The matrices T and U are the images of the original matrices.
The commutative diagram shows how the method works.
It finds the maps W and C from the spaces X and Y to the hidden spaces T and U, as well as the inverse mapping P and Q. 
We propose to concord the projections in the hidden space.
In this case, we find the link operator B in the form of a linear model for concording hidden spaces.
The final decoding model is linear, but has a low-dimensional hidden representation.

Slide 5: 
We solve the problem in the assumption of high redundancy of the data description.
To find the optimal low-dimensional manifold, we introduce a hidden space notion.
The general scheme of concording dependencies for the decoding problem is shown in the following commutative diagram.
If there exists invertible encoding functions phi_e and psi_e and decoding functions phi_d and psi_d, we will say that for the input and target spaces X and Y, there are hidden spaces T and U.
In this case, hidden spaces are concordant if there is a link function h, which maps objects from one hidden space to another.
To find the encoding and decoding functions, as well as the link function, the task of projection concordance is posed as maximizing the concordance function g.

Slide 6:
We propose the iterative procedure for finding projections with maximum covariance.
A theorem on the optimality for model parameters of the concordant decoding model has been proved in the case of using the covariance between the projections t and u as a concordance function g.

Slide 7:
Here on the slide one can see an example of how the projection method works in a hidden two-dimensional space.
The blue and green dots represent the input variables x and the target variables y.
The input variables are generated from a normal distribution with some Sigma covariance matrix.
The target variables depend linearly on the second principal component and do not depend on the first principal component of the matrix X.
The red contour shows the level lines of the covariance matrices distribution.
The black arrows correspond to the vectors of the matrices W and C.
When we reduce the dimensionality of the input space without taking into account the existing dependencies in the target space using the principal component method, we get an inadequate solution.
Using a concordant hidden space, the model is able to find the optimal hidden subspace of dimension 1, given by the operators W and C. This takes into account the interdependence between the images of the X and Y matrices.

Slide 8:
To build a model selection procedure, let consider the case of an additive superposition of decoding models.
The overall decoding model is the sum of two separate models.
The goal is to obtain an algorithm that answers the question about the need to include a specific model in the general superposition.
We have proved the statement about optimal parameters of each of the models in a superposition.
And also we have proved the theorem on the optimality of the model superposition over each particular decoding model.
A procedure for selecting models in a superposition is proposed, based on the analysis of subspace projections constructed on linear span of the original data descriptions.

Slide 9:
In addition to linear decoding models, we consider nonlinear methods.
In this case, the encoding and decoding functions are deep neural networks.
Each neural network is a superposition of matrixx multiplications and elementwise activation functions.
The concordance process consists in maximizing the concordance function according to the parameters of the neural networks.
Using neural networks, the decoding model is able to take into account significantly nonlinear dependencies both in the input space and in the target space.
If correlation is used as a concordance function, we could get expression for the gradient of the concordance function.
This expression for the gradient allows us to build an effective algorithm for solving the problem using gradient optimization methods.

Slide 10:
Для получения простой модели и анализа значимостей конкретных признаков рассматривается задача выбора оптимального подмножества признаков.
В задаче выбора признаков требуется найти бинарный вектор a, компоненты которого являются индикаторами выбранных признаков.
Вводится функция ошибки S для задачи выбора признаков.
Вектор z содержит значения значимости исходных признаков.
Решение о включении признака в финальную модель принимается по отсечению значимости по порогу.
Функция ошибки модели декодирования для данного случая учитывает только выбранные признаки.
Для анализа зависимостей рассматривается фильтрационный метод выбора признаков с помощью квадратичного программирования.
Данный алгоритм находит подмножество признаков с помощью квадратичной функции S, минимизируя парные взаимодействия исходных признаков и максимизируя релевантность исходных признаков к целевой переменной.

Слайд 11:
Доказано, что при использовании в качестве полуопределенной релаксации сдвига спектра матрицы парных взаимодействий Q, квадратичная задача выбора признаков имеет единственный глобальный минимум.
Сложность рассматриваемой задачи заключается в учёте зависимостей в целевом пространстве.
Для обобщения используемого метода на случай векторной целевой переменной в качестве базовой стратегии используется метод агрегирования релевантностей по целевым векторам.
Таким образом вектор релевантностей b получается суммированием корреляций по всем целевым векторам.
Недостаток данного метода заключается в отсутствии учёта зависимостей в пространстве целевой матрицы.

Слайд 12:
Для учёта данных зависимостей предлагаются обобщения исходного метода.
Симметричный учёт значимостей штрафует коррелированные целевые вектора с помощью введения матрицы парных взаимодействий Q_y для целевых векторов. 
Таким образом кроме учёта взаимодействий исходных признаков, функция ошибки выбора признаков учитывает взаимодействия целевых векторов.
Симметричный учёт значимостей приводит к малым значениям значимостей для целевых векторов, которые слабо коррелируют с признаками, и большим значениям для целевых векторов, которые сильно коррелируют с признаками. 
Для учёта целевых векторов, слабо коррелирующих с исходными признаками, предлагается добавление штрафующего члена для целевых векторов, которые коррелируют с признаками.
Вклад признаков и целевых векторов становится асимметричным.

Слайд 13: 
Если в задаче есть целевые вектора, которые слабо коррелируют с исходными признаками, предлагается минимаксная задача выбора признаков. 
Доказано, что в случае положительно определенных матриц парных взаимодействий мин макс и макс мин задачи имеют одинаковое решение.
Также доказано, что минимаксная задача эквивалентна задаче квадратичного программирования.
Для получения выпуклой задачи применяется полуопределенная релаксация.

Слайд 14:
Помимо базового метода, предлагается 3 метода выбора признаков для учёта зависимостей в целевом пространстве. 
Показано, что все предлагаемые стратегии являются обобщениями исходного метода для случая скалярной целевой переменной.

Слайд 15:
Также в работе предлагается алгоритм оптимизации нелинейных моделей с помощью рассмотренного метода квадратичного программирования.
Для этого вводится понятие активного параметра модели.
Активным считается параметр,  при уточнении которого  с помощью градиентного метода второго порядка, функция ошибки может уменьшиться.
Доказана теорема о виде функции ошибки выбора активных параметров модели при использовании для оптимизации нелинейной модели метода Ньютона.
На слайде изображен модельный двумерный пример выбора единственного активного параметра при оптимизации модели.

Слайд 16:
Для прикладного анализа предложенных методов вводятся внешние критерии качества. Нормированная среднеквадратичная ошибка говорит о качестве прогноза.
Коэффициент мультикорреляции отвечает за устойчивость финальной модели.
Байесовский информационный критерий является мерой сложности модели.

Слайд 17:
В качестве прикладной задачи рассматривается задача построения нейрокомпьютерного интерфейса.
Требуется построить прогностическую модель декодирования, восстанавливающую траекторию конечности по сигналам электрокортикограммы.
Как пространство исходных сигналов, так и пространство целевых сигналов обладают высокой размерностью, которая является избыточной.
Исходные сигналы электрокортикограммы формируют матрицу X. 
Целевые сигналы траектории движения конечности формируют авторегрессионную матрицу Y. Компоненты целевой переменной сильно скоррелированы по временной оси.

Слайд 18: 
На слайде показаны графики поведения описанных внешних критериев для всех рассматриваемых методов.
Предложенные методы имеют меньшую ошибку, являются более устойчивыми и выбирают более простую модель по отношению к базовому алгоритму.

Слайд 19:
Приводится сравнение метода проекции в скрытое пространство с методами выбора признаков.
Слева на слайде показано, что для данной задачи оптимальная размерность скрытого пространства существенно меньше размерности исходных описаний данных.
Предложенные методы достигают меньшей ошибки по сравнению с известными ранее методами.
Наилучший результат достигается комбинацией двух подходов.

Слайд 20: 
На защиту выносятся методы декодирования сигналов, учитывающие зависимости как в исходном, так и в целевом пространстве, методы выбора согласованных моделей в случае избыточной размерности пространств.
Доказаны теоремы об оптимальности предлагаемых методов.
Предложены методы выбора признаков, которые доставляют устойчивые и адекватные решения в коррелированных пространствах высокой размерности.
Предложены нелинейные методы согласования скрытых пространств.
Предложенные модели позволяют построить эффективную систему прогнозирования гетерогенных наборов данных для задачи построения нейрокомпьютерного интерфейса.

Слайд 21:
По теме диссертации было опубликовано 7 работ, 6 в журналах из списка ВАК. Благодарю за внимание. 
