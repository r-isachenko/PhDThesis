Slide 1:
Hello, everyone! My name is Roman Isachenko. I am PhD student at MIPT. The topic of my today talk is Dimensionality Reduction for Signal Decoding.

Slide 2:
In this study we are considering the problem of selection of optimal decoding model.
The decoding process essentially is a process restoring the dependencies between two heterogeneous data sets.
The challenge is that we consider the case where target variable is a vector.
The spaces of the input signals and target signals have excessive dimensionality.
Dimensionality reduction methods that do not take into account dependencies in the target space are not adequate.
We propose to reduce the dimensionality of the input and target spaces by projecting signals into latent spaces of significantly smaller dimensionality.
Linear and nonlinear methods for building concordant models are proposed.

Slide 3:
Our goal is to build a decoding model f, which takes the input signal x and predicts the target signal y.
The slide shows an example of the input and target time series for the task of restoring the hand trajectory based on the signals of the electrocorticogram.
The input and target signals have a different nature and have a high dimensionality.
A point in the x-space is a segment of the input signal from the space of dimension n.
A point in the target space y is a segment of the target signal from the space of dimensionality r.
The complexity of the problem lies in the fact that at each moment of time t, the forecasting decoding model have to predict a vector consisting of successive components of the target signal.
To construct an adequate model f, a procedure for dependencies concordance  in a latent space is proposed.
In order to do this, the input and target signals are projected into the latent space, which has a smaller dimensionality compared to the dimensionality of the original data description.
The operators P and Q restore the input and target variables from the latent space projections.
At the same time, it is proposed to concord the projections in such a way that the covariance between the projections is maximal.

Slide 4:
Let consider the case of a linear signal decoding model.
The input and target variables form the X and Y matrices.
The model parameters Theta are a matrix.
We will use the simple quadratic l2 loss function between the target variable and the forecast for this decoding model.
If there is a strong multicorrelation between the columns of the matrix of the frequency domain of the input signals, the decoding model turns out to be unstable.
In this moment we propose a method of projection data descriptions into a latent space.
The original matrices X and Y are projected into latent spaces.
The matrices T and U are the images of the original matrices.
The commutative diagram shows how the method works.
It finds the maps W and C from the spaces X and Y to the latent spaces T and U, as well as the inverse mapping P and Q. 
We propose to concord the projections in the latent space.
In this case, we find the link operator B in the form of a linear model for concording latent spaces.
The final decoding model is linear, but has a low-dimensional latent representation.

Slide 5: 
We solve the problem in the assumption of high redundancy of the data description.
To find the optimal low-dimensional manifold, we introduce a latent space notion.
The general scheme of concording dependencies for the decoding problem is shown in the following commutative diagram.
If there exists invertible encoding functions phi_e and psi_e and decoding functions phi_d and psi_d, we will say that for the input and target spaces X and Y, there are latent spaces T and U.
In this case, latent spaces are concordant if there is a link function h, which maps objects from one latent space to another.
To find the encoding and decoding functions, as well as the link function, the task of projection concordance is posed as maximizing the concordance function g.

Slide 6:
We propose the iterative procedure for finding projections with maximum covariance.
A theorem on the optimality for model parameters of the concordant decoding model has been proved in the case of using the covariance between the projections t and u as a concordance function g.

Slide 7:
Here on the slide one can see an example of how the projection method works in a latent two-dimensional space.
The blue and green dots represent the input variables x and the target variables y.
The input variables are generated from a normal distribution with some Sigma covariance matrix.
The target variables depend linearly on the second principal component and do not depend on the first principal component of the matrix X.
The red contour shows the level lines of the covariance matrices distribution.
The black arrows correspond to the vectors of the matrices W and C.
When we reduce the dimensionality of the input space without taking into account the existing dependencies in the target space using the principal component method, we get an inadequate solution.
Using a concordant latent space, the model is able to find the optimal latent subspace of dimension 1, given by the operators W and C. This takes into account the interdependence between the images of the X and Y matrices.

Slide 8:
To build a model selection procedure, let consider the case of an additive superposition of decoding models.
The overall decoding model is the sum of two separate models.
The goal is to obtain an algorithm that answers the question about the need to include a specific model in the general superposition.
We have proved the statement about optimal parameters of each of the models in a superposition.
And also we have proved the theorem on the optimality of the model superposition over each particular decoding model.
A procedure for selecting models in a superposition is proposed, based on the analysis of subspace projections constructed on linear span of the original data descriptions.

Slide 9:
In addition to linear decoding models, we consider nonlinear methods.
In this case, the encoding and decoding functions are deep neural networks.
Each neural network is a superposition of matrixx multiplications and elementwise activation functions.
The concordance process consists in maximizing the concordance function according to the parameters of the neural networks.
Using neural networks, the decoding model is able to take into account significantly nonlinear dependencies both in the input space and in the target space.
If correlation is used as a concordance function, we could get expression for the gradient of the concordance function.
This expression for the gradient allows us to build an effective algorithm for solving the problem using gradient optimization methods.

Slide 10:
To get a simple model and analyze the importance of specific features, let consider the problem of choosing the optimal feature subset.
The goal of the feature selection problem is to find a binary vector a, whose components are indicators of the selected features.
We introduce the loss function S for the feature selection problem.
The vector z contains the importance values of the initial features.
The decision to include a feature in the final model is made by cutting off the importances by the threshold.
The loss function of the decoding model for this case takes into account only the selected features.
To analyze the feature dependencies, we consider  filter method for feature selection features using quadratic programming approach.
This method finds a feature subset using a quadratic function S, minimizing the pairwise similarities of the original features and maximizing the relevances of the original features to the target variable.

Slide 11:
We have proved that when using the spectrum shift of the similarity matrix Q as a semi-definite relaxation, the quadratic feature selection problem has a unique global minimum.
The complexity of the problem that we consider is to take into account dependencies in the target space.
To generalize the method to the case of a vector target variable, we use the method of aggregating relevancies by target vectors as a basic strategy.
Thus, the relevance vector b is obtained by sumation of correlations across all target vectors.
The disadvantage of this method is the lack of accounting for dependencies in the space of the target matrix Y.

Slide 12:
We propose generalizations of the original method to account for these dependencies.
Symmetric importance strategy penalizes correlated target vectors by introducing a matrix of pairwise similarities Q_y for target vectors.
Thus, in addition to taking into account the interactions of the initial features, the feature selection loss function takes into account the interactions of the target vectors.
Symmetric importance strategy leads to small values of importances for target vectors that are weakly correlated with features, and large values for the target vectors that are strongly correlated with the features.
To account for the target vectors that are weakly correlated with the initial features, it is proposed to add a penalty term for the target vectors that correlate with the features.
The contribution of features and target vectors becomes asymmetric.

Slide 13: 
If we have target vectors that are weakly correlated with the initial features, a minimax feature selection problem is proposed.
It has been proved that in the case of positive definite matrices of pairwise similarities minmax and maxmin problems have the same solution.
It has also been proved that the minimax problem is equivalent to the quadratic programming problem.
To obtain a convex problem, a semi-definite relaxation could be used.

Slide 14:
To sum up, In addition to the basic method, we have proposed 3 methods for feature selection to account for dependencies in the target space.
It has been shown that all the proposed strategies are generalizations of the original method for the case of a scalar target variable.

Slide 15:
We also propose an algorithm for optimizing nonlinear models using the considered quadratic programming method.
For this purpose, we introduce the concept of the active parameter of the model.
An active parameter is a parameter that is able to decrease the loss function using the second-order gradient optimization method.
We have proved the theorem on the form of the loss function for the selection of the active model parameters when using the Newton method for optimization of a nonlinear model.
The slide shows a two-dimensional model example of choosing a single active parameter when optimizing the model.

Slide 16:
For the practical analysis of the proposed methods, we introduce the following quality criteria. 
The normalized error indicates the quality of the forecast.
The multicorrelation coefficient is responsible for the stability of the final model.
The Bayesian information criterion is a measure of the complexity of the model.

Slide 17:
We consider the problem of building a brain-computer interface as an applied problem.
The goal is to build a forecasting decoding model that restores the hand trajectory from the signals of the electrocorticogram.
Both the space of the input signals and the space of the target signals have a excessive dimensionality, which is redundant.
The initial signals of the electrocorticogram form the matrix X.
The target signals of the hand trajectory form an autoregressive matrix Y. The components of the target variable are strongly correlated along the time axis.

Slide 18: 
On this slide we show the described quality criteria for all the methods that we consider.
The proposed methods have a smaller error, are more stable and choose a simpler model in relation to the basic algorithm.

Slide 19:
We also compare the method of projection to the latent space is compared with the methods of feature selection.
The picture on the left shows that for this problem, the optimal dimensionality of the latent space is significantly less than the dimensionality of the original data descriptions.
The proposed methods achieve a smaller error compared to well known methods.
The best result is achieved by a combination of two approaches.

Slide 20: 
To conclude, we have propsoed the dethods for signal decoding that take into account dependencies in both the input and target spaces and methods for selecting concordant models in the case of excessive dimensionality of spaces.
We have proved the theorems on the optimality of the proposed methods.
We also have proposed the methods for feature selection that gives stable and adequate solutions in the correlated spaces of high dimensionality.
We gave got nonlinear methods for concording latent spaces.
The proposed models allow us to build an effective forecasting system for heterogeneous data sets, for example, for the buildinng brain-computer interface.

Slide 21:
Thank you for your attention.
