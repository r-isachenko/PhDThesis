% !TEX encoding = UTF-8 Unicode
\documentclass[a4paper,12pt]{article}
	
	% В этом документе преамбула
	
	%%% Работа с русским языком
	\usepackage{cmap}					% поиск в PDF
	\usepackage{mathtext} 				% русские буквы в формулах
	\usepackage[T2A]{fontenc}			% кодировка
	\usepackage[utf8]{inputenc}			% кодировка исходного текста
	\usepackage[english,russian]{babel}	% локализация и переносы
	\usepackage{indentfirst}
	\frenchspacing
	
	\renewcommand{\epsilon}{\ensuremath{\varepsilon}}
	\renewcommand{\phi}{\ensuremath{\varphi}}
	\renewcommand{\kappa}{\ensuremath{\varkappa}}
	\renewcommand{\le}{\ensuremath{\leqslant}}
	\renewcommand{\leq}{\ensuremath{\leqslant}}
	\renewcommand{\ge}{\ensuremath{\geqslant}}
	\renewcommand{\geq}{\ensuremath{\geqslant}}
	\renewcommand{\emptyset}{\varnothing}
	
	%%% Дополнительная работа с математикой
	\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools} % AMS
	\usepackage{icomma} % "Умная" запятая: $0,2$ --- число, $0, 2$ --- перечисление
	
	%% Номера формул
	%\mathtoolsset{showonlyrefs=true} % Показывать номера только у тех формул, на которые есть \eqref{} в тексте.
	%\usepackage{leqno} % Нумереация формул слева
	
	%% Свои команды
	\DeclareMathOperator{\sgn}{\mathop{sgn}}
	
	%% Перенос знаков в формулах (по Львовскому)
	\newcommand*{\hm}[1]{#1\nobreak\discretionary{}
		{\hbox{$\mathsurround=0pt #1$}}{}}
	
	%%% Работа с картинками
	\usepackage{graphicx}  % Для вставки рисунков
	\setlength\fboxsep{3pt} % Отступ рамки \fbox{} от рисунка
	\setlength\fboxrule{1pt} % Толщина линий рамки \fbox{}
	\usepackage{wrapfig} % Обтекание рисунков текстом
	
	%%% Работа с таблицами
	\usepackage{array,tabularx,tabulary,booktabs} % Дополнительная работа с таблицами
	\usepackage{longtable}  % Длинные таблицы
	\usepackage{multirow} % Слияние строк в таблице
	
	%%% Теоремы
	\theoremstyle{plain} % Это стиль по умолчанию, его можно не переопределять.
	\newtheorem{theorem}{Theorem}
	\newenvironment{Proof} % имя окружения
	{\par\noindent{\bf Proof.}} % команды для \begin
	{\hfill$\scriptstyle\blacksquare$} % команды для \end
	
	\newtheorem{proposition}[theorem]{Утверждение}
	
	\theoremstyle{definition} % "Определение"
	\newtheorem{corollary}{Следствие}[theorem]
	\newtheorem{problem}{Задача}[section]
	
	\theoremstyle{remark} % "Примечание"
	\newtheorem*{nonum}{Решение}
	
	%%% Программирование
	\usepackage{etoolbox} % логические операторы
	
	%%% Страница
	\usepackage{extsizes} % Возможность сделать 14-й шрифт
	\usepackage{geometry} % Простой способ задавать поля
	\geometry{top=25mm}
	\geometry{bottom=35mm}
	\geometry{left=35mm}
	\geometry{right=20mm}
	%
	%\usepackage{fancyhdr} % Колонтитулы
	% 	\pagestyle{fancy}
	%\renewcommand{\headrulewidth}{0pt}  % Толщина линейки, отчеркивающей верхний колонтитул
	% 	\lfoot{Нижний левый}
	% 	\rfoot{Нижний правый}
	% 	\rhead{Верхний правый}
	% 	\chead{Верхний в центре}
	% 	\lhead{Верхний левый}
	%	\cfoot{Нижний в центре} % По умолчанию здесь номер страницы
	
	\usepackage{setspace} % �?нтерлиньяж
	%\onehalfspacing % �?нтерлиньяж 1.5
	%\doublespacing % �?нтерлиньяж 2
	%\singlespacing % �?нтерлиньяж 1
	
	\usepackage{lastpage} % Узнать, сколько всего страниц в документе.
	
	\usepackage{soul} % Модификаторы начертания
	
	\usepackage{hyperref}
	\usepackage[usenames,dvipsnames,svgnames,table,rgb]{xcolor}
	\hypersetup{				% Гиперссылки
		unicode=true,           % русские буквы в раздела PDF
		pdftitle={Заголовок},   % Заголовок
		pdfauthor={Автор},      % Автор
		pdfsubject={Тема},      % Тема
		pdfcreator={Создатель}, % Создатель
		pdfproducer={Производитель}, % Производитель
		pdfkeywords={keyword1} {key2} {key3}, % Ключевые слова
		colorlinks=true,       	% false: ссылки в рамках; true: цветные ссылки
		linkcolor=red,          % внутренние ссылки
		citecolor=black,        % на библиографию
		filecolor=magenta,      % на файлы
		urlcolor=blue           % на URL
	}
	
	\usepackage{csquotes} % Еще инструменты для ссылок
	
	%\usepackage[style=authoryear,maxcitenames=2,backend=biber,sorting=nty]{biblate x}
	
	\usepackage{multicol} % Несколько колонок
	
	\usepackage{tikz} % Работа с графикой
	\usepackage{pgfplots}
	\usepackage{pgfplotstable}
	\usepackage{caption}
	\usepackage{subcaption}
	
	\usepackage{algorithm}
	\usepackage[noend]{algcompatible}
	
	\newcommand{\ba}{\mathbf{a}}
	\newcommand{\bb}{\mathbf{b}}
	\newcommand{\bw}{\mathbf{w}}
	\newcommand{\by}{\mathbf{y}}
	\newcommand{\bx}{\mathbf{x}}
	\newcommand{\bz}{\mathbf{z}}
	\newcommand{\cA}{\mathcal{A}}
	\newcommand{\bJ}{\mathbf{J}}
	\newcommand{\bQ}{\mathbf{Q}}
	\newcommand{\bbC}{\mathbb{C}}
	\newcommand{\bbR}{\mathbb{R}}
	\newcommand{\bbY}{\mathbb{Y}}
	\newcommand{\bW}{\mathbf{W}}
	\newcommand{\bH}{\mathbf{H}}
	\newcommand{\bF}{\mathbf{F}}
	\newcommand{\bR}{\mathbf{R}}
	\newcommand{\bX}{\mathbf{X}}
	
	\newcommand{\T}{{\text{\tiny\sffamily\upshape\mdseries T}}}
	\newcommand{\argmin}{\mathop{\arg \min}\limits}
	\newcommand{\argmax}{\mathop{\arg \max}\limits}
	
	\begin{document}
		
		\title
	{Выбор параметров оптимизации с помощью квадратичного программирования для метода Ньютона}
	\date{}
	\maketitle
	\begin{center}
		Р.\,В.~Исаченко,
		В.\,В.~Стрижов
	\end{center}
	\textbf{Аннотация:} 
	Статья посвящена проблеме построения прогностической модели в многомерном признаковом пространстве.
	Пространство избыточно, столбцы матрицы объектов мультиколлинеарны.
	В этом случае модель неустойчива к изменениям данных или значений параметров. 
	Для построения устойчивой модели решается задача снижения размерности пространства признаков.
	Предлагается использовать методы выбора признаков в процессе оптимизации параметров.
	Идея заключается в выборе активного набора параметров модели для оптимизации на текущем шаге процедуры оптимизации.
	Метод выбора признаков с помощью квадратичного программирования используется для поиска активного набора параметров. 
	Алгоритм максимизирует релевантность параметров модели невязкам прогнозирования и максимизирует попарную независимость параметров. 
	Исследуются модели нелинейной и логистической регрессии. 
	Проведён вычислительный эксперимент, который показывает, как работает предложенный метод и сравнить его с другими методами оптимизации. 
	Предложенный алгоритм достигает меньшей ошибки и большей стабильности по сравнению с другими методами.
	
	\bigskip
	\textbf{Ключевые слова}: quadratic programming feature selection, nonlinear regression, logistic regression, Newton method
	
	\section*{Введение}
	Функция ошибки для моделей с большим числом параметров имеет сложный ландшафт с множеством локальных минимумов. 
	В этом случае алгоритм оптимизации каждый раз даёт разные оптимальные решения.
	
	Алгоритм оптимизации представляет собой итеративный процесс. 
	На каждом шаге он обновляет текущие параметры для получения следующего приближения.
	Алгоритмы оптимизации первого порядка используют первые производные функции ошибок для обновления параметров. 
	Наиболее известными алгоритмами первого порядка являются алгоритм градиентного спуска, метод момента Нестерова~{nesterov1983momentum}, Adagrad~\cite{duchi2011adagrad}, Adam~\cite{kingma2014adam}. 
	Эти алгоритмы используются для оптимизации глубоких нейронных сетей~\cite{goodfellow2016deeplearningbook}. 
	Алгоритм Ньютона является алгоритмом второго порядка, который использует вторые производные функции ошибки. 
	Алгоритм находит вектор обновления параметров, используя квадратичную аппроксимацию функции ошибки, и сходится за меньше число итераций.
	Недостатком методов оптимизации второго порядка является их масштабируемость.
	При большом числе параметров матрица Гессиана оказывается плохо обусловленной. 
	Процесс оптимизации в данном случае является вычислительно затратным и может расходится. 
	Авторы~\cite{avriel2003nonlinear, blaschke1997convergence} предлагают методы аппроксимации для матрицы Гессиана и наложение регуляризации для преодоления этой проблемы.
	В статье~\cite{botev2017newtondeeplearning} метод Ньютона применяется к глубоким нейронным сетям.
	
	В данной работе предлагается метод выбора параметров модели, которые оптимизируются на текущем шаге оптимизации.
	Авторы предлагают внедрить процедуру выбора признаков в процесс оптимизации.
	Обширный обзор алгоритмов выбора признаков можно найти в~\cite{li2017feature}.
	Для выбора подмножества некоррелированных признаков, релевантных целевой переменной в~\cite{ding2005mrmr,yamada2014mrmr,peng2005feature} был предложен подход максимальной релевантности при минимальной избыточности~(mRMR).
	Алгоритм выбора признаков с помощью квадратичного программирования~(QPFS)~\cite{katrutsa2017comprehensive,rodriguez2010qpfs} сводит задачу mRMR к задаче квадратичного программирования. Эта задача может быть эффективно решена с помощью современных программных пакетов для оптимизации~\cite{bertsimas2016best}.
	
	Рассматриваются модель нелинейной регрессии с квадратичной функцией ошибок и модель логистической регрессии с функцией ошибкой в виде кросс-энтропии.
	Для нелинейной регрессии метод Ньютона и линеаризация модели приводят к методу Гаусса-Ньютона. 
	На каждом шаге решается задача линейной регрессии. 
	В качестве нелинейной модели используется двухслойная нейронная сеть. 
	Метод Ньютона для логистической регрессии сводится к итеративному алгоритму взвешенных наименьших квадратов~(IRLS). 
	Здесь шаг оптимизации делается в направлении, заданном решением задачи линейной регрессии.
	В работе предлагается применить алгоритм QPFS~\cite{katrutsa2017comprehensive, rodriguez2010qpfs} для выбора оптимального набора параметров модели. В обоих случаях на каждом шаге приходится решить задачу линейной регрессии. 
	Алгоритм QPFS позволяет найти подмножество независимых параметров, оказывающих наибольший вклад на вектор остатков.
	
	В вычислительном эксперименте исследуется поведение предложенного алгоритма вблизи точки оптимума.
	Алгоритм сравнивается с другими методами, такими как градиентный спуск, момент Нестерова, Adam и алгоритм Ньютона. 
	
	Основной вклад данной работы:
	\begin{itemize}
		\item решается задача выбора параметров для оптимизации модели с помощью квадратичного программирования; 
		\item оценивается эффективности предложенного алгоритма на тестовом наборе данных;
		\item проводится сравнение предложенного алгоритма с другими алгоритмами на реальных наборах данных, предложенный алгоритм превосходит другие методы.
	\end{itemize}
	
	\section*{Постановка задачи}
	
	Модель $f( \bx, \bw)$ с параметрами $\bw \in \mathbb{R}^p$ предсказывает целевую переменную~$y \in \bbY$ по объекту~$\bx \in \bbR^{n}$. Пространство~$\bbY$ представляет собой бинарные метки классов~$\{0, 1\}$ для задачи двухклассовой классификации и~$\bbR$ для задачи регрессии.
	Даны матрица плана~$\bX = [\bx_1, \dots, \bx_m]^{\T} \in \bbR^{m \times n}$ и целевой вектор~$\by = [y_1, \dots, y_m]^{\T} \in \bbY^{m}$. 
	Цель состоит в нахождении оптимальных параметров~$\bw^*$.
	Параметры~$\bw$ вычисляются минимизацией функции ошибки:
	\begin{equation}
	\bw^* = \argmin_{\bw \in \bbR^p} S(\bw | \bX, \by, f).
	\label{eq:error_function}
	\end{equation}
	В качестве функции ошибки~$S (\bw / \bX, \by, f)$ рассматриваются квадратичная ошибка для задачи регрессии:
	\begin{equation}
	S(\bw | \bX, \by, f) = \frac 12 \| \by - \mathbf{f}(\bX, \bw) \|_2^2 = \frac 12 \sum_{i=1}^m \bigl( y_i - f(\bx_i,  \bw)\bigr)^2,
	\label{eq:squared_error}
	\end{equation}
	и функция кросс-энтропии для задачи бинарной классификации: 
	\begin{equation}
	S(\bw | \bX, \by, f) = \sum_{i=1}^m \bigl[y_i \log f (\bx_i , \bw) + (1-y_i) \log \bigl(1 - f (\bx_i , \bw)\bigr)\bigr].
	\label{eq:log_loss}
	\end{equation}
	
	Задача~\eqref{eq:error_function} решается с помощью итеративной процедуры оптимизации. 
	Для получения параметров на шаге~$k$ текущие параметры $\bw^{k-1}$ обновляются по следующему правилу:
	\begin{equation}
	\bw^k = \bw^{k - 1} + \Delta \bw^{k - 1}.
	\label{eq:update_rule}
	\end{equation}
	Авторы используют метод оптимизации Ньютона для выбора вектора обновлений~$\Delta \bw$.
	
	Метод Ньютона нестабилен и вычислительно сложен. 
	В данной статье предлагается стабильный алгоритм Ньютона. 
	Перед шагом градиента предлагается выбрать подмножество активных параметров модели, которые оказывают наибольшее влияние на функцию ошибки~$S (\bw)$.
	Обновление параметров производится только для отобранного множества индексов~$\cA \subseteq \{ 1, \dots, p \}$
	
	\begin{align*}
	\bw_{\cA}^k &= \bw_{\cA}^{k - 1} + \Delta \bw_{\cA}^{k - 1}, \quad \bw_{\cA} = \{w_j\}_{j \in \cA}; \\
	\bw_{\bar{\cA}}^k &= \bw_{\bar{\cA}}^{k - 1}, \quad \bw_{\bar{\cA}} = \{w_j\}_{j \notin \cA}.
	\end{align*}
	Чтобы выбрать подмножество~$\cA$ из всех возможных $2^p - 1$~подмножеств, вводится функция качества
	\begin{equation}
	\cA = \argmax_{\cA' \subseteq \{1, \dots, p\}} Q(\cA' | \bX, \by, f, \bw).
	\label{eq:subset_selection}
	\end{equation}
	Задача~\eqref{eq:subset_selection} решается на каждом шаге $k$ процесса оптимизации для текущих параметров~$\bw^k$.
	
	\section*{Выбор признаков с помощью квадратичного программирования}
	Если между столбцами матрицы плана~$\bX$ существует мультиколлинеарность, то решение задачи линейной регрессии
	\begin{equation}
	\| \by - \bX \bw\|_2^2 \rightarrow\min_{\bw \in \bbR^{n}}.
	\label{eq:linear_regression}
	\end{equation}
	оказывается неустойчивым. 
	Методы выбора признаков находят подмножество~$ \cA \in \{1, \dots, n\}$ оптимальных столбцов~$\bX$. 
	
	Алгоритм QPFS выбирает некоррелированные признаки, релевантные целевому вектору~$\by$.
	Чтобы формализовать этот подход, введем две функции: $\text{Sim}(\bX)$ и $\text{Rel}(\bX, \by)$. 
	$\text{Sim}(\bX)$ контролирует избыточность между признаками, $\text{Rel}(\bX, \by)$ содержит релевантности между каждым признаком и целевым вектором. 
	Мы хотим минимизировать функцию Sim и максимизировать Rel одновременно.
	
	QPFS предлагает явный способ построения функций Sim и Rel. 
	Алгоритм минимизирует следующую функцию ошибки
	\begin{equation}
	\underbrace{\ba^{\T} \bQ \ba}_{\text{Sim}} - \alpha \cdot \underbrace{\vphantom{()} \mathbf{b}^{\T} \ba}_{\text{Rel}} \rightarrow \min_{\substack{\ba \in \bbR^n_+ \\ \|\ba\|_1=1}}.
	\label{eq:quadratic_problem}
	\end{equation}
	Элементы матрицы~$\bQ \in \bbR^{n \times n}$ содержат коэффициенты попарного сходства между признаками. 
	Вектор~$\mathbf{b} \in \bbR^n$ выражает сходство между каждым признаком и целевым вектором~$\by$.
	Нормированный вектор~$\ba$ отражает важность каждого признака. 
	Функция ошибки~\eqref{eq:quadratic_problem} штрафует зависимые признаки функцией Sim и штрафует признаки, не релевантные к целевой переменной функцией Rel. 
	Параметр~$\alpha$ позволяет контролировать компромисс между функциями Sim и Rel.
	Авторы оригинальной в статье о QPFS предложили способ выбора~$\alpha$, чтобы уравновесить вклад членов $\text{Sim}(\bX)$ и $\text{Rel}(\bX, \by)$
	
	\begin{equation*}
	\alpha = \frac{\overline{\bQ}}{\overline{\bQ} + \overline{\bb}},
	\end{equation*}
	где $\overline{\bQ}$, $\overline{\bb}$~-- средние значения~$\bQ$ и $\bb$ соответственно.
	Чтобы выделить оптимальное подмножество признаков, применяется отсечение по порогу:
	\[
	j \in \mathcal{A} \Leftrightarrow a_j > \tau.
	\]
	
	Для измерения сходства используется выборочный коэффициент корреляции между парами признаков для функции Sim, и между признаками и целевым вектором для функции Rel.
	Задача~\eqref{eq:quadratic_problem} является выпуклой, если матрица~$\bQ$ является неотрицательно определенной. В общем случае это не всегда верно. 
	Чтобы удовлетворить этому условию спектр матрицы~$\bQ$ смещается, и матрица~$\bQ$ заменяется на $\bQ - \lambda_{\text{min}} \mathbf{I}$, где $\lambda_{\text{min}} $ является минимальным собственным значением~$\bQ$.
	
	Алгоритм QPFS используется для решения задачи~\eqref{eq:subset_selection}.
	QPFS выбирает подмножество~$\cA$ для вектора обновлений~$ \Delta \bw$, которые оказывают наибольшее влияние на вектор остатков и являются попарно независимыми.
	Функция ошибки~\eqref{eq:quadratic_problem} соответствует критерию качества~$Q(\cA | \bX, \by, f, \bw)$
	\begin{equation}
	\cA = \argmax_{\cA' \subseteq \{1, \dots, p\}} Q(\cA' | \bX, \by, f, \bw) \Leftrightarrow \argmin_{\ba  \in \bbR^p_+, \, \|\ba\|_1=1} \bigl[\ba^{\T} \bQ \ba - \alpha \cdot \mathbf{b}^{\T} \ba \bigr].
	\end{equation}
	В работе показано, что для модели нелинейной регрессии с квадратичной функцией ошибки~\eqref{eq:squared_error} и для модели логистической регрессии с кросс-энтропией~\eqref{eq:log_loss}, каждый шаг оптимизации эквивалентен задаче линейной регрессии~\eqref{eq:linear_regression}.

	\section*{Метод Ньютона}
	Метод Ньютона использует условие оптимизации первого порядка для задачи~\eqref{eq:error_function} и линеаризует градиент $S (\bw)$
	\[
	\nabla S (\bw + \Delta \bw) = \nabla S(\bw) + \bH \cdot \Delta \bw = 0,
	\]
	\[
	\Delta \bw = - \bH^{-1} \nabla S(\bw).
	\]
	где $\bH = \nabla^2 S(\bw)$ является Гессианом матрицы функции ошибки $S(\bw)$.
	
	Итерация~\eqref{eq:update_rule} метода Ньютона~--
	\[
	\bw^k = \bw^{k-1} - \bH^{-1} \nabla S(\bw).
	\]
	Каждая итерация инвертирует матрицу Гессиана.
	Мерой плохой обусловленности для матрицы Гессиана~$\bH$ является число обусловленности
	\[
	\kappa(\bH) = \frac{\lambda_{\text{max}}(\bH)}{\lambda_{\text{min}}(\bH)},
	\]
	где $\lambda_{\text{max}}(\bH), \lambda_{\text{min}}(\bH)$ являются максимальным и минимальным собственными значениями~$\bH$. Большое число обусловленности~$\kappa (\bH)$ приводит к нестабильности процесса оптимизации.
	Предложенный алгоритм уменьшает размер матрицы Гессиана~$\bH$. В наших экспериментах это приводит к меньшему числу обусловленности~$\kappa (\bH)$.
	
	Размер шага метода Ньютона может быть чрезмерно большим. Для управления размером шага обновлений добавим параметр $\eta$ в правило обновления~\eqref{eq:update_rule}
	\[
	\bw^k = \bw^{k - 1} + \eta \Delta \bw^{k - 1}, \quad \eta \in [0, 1].
	\]
	
	Для выбора соответствующего размера шага~$\eta$ используется правило Армихо. Выбирается максимальное~$\eta$ так, чтобы выполнялось следующее условие
	\[
	S(\bw^{k - 1} + \eta \Delta \bw^{k - 1}) < S(\bw^{k - 1}) + \gamma \eta \nabla S^{\T}(\bw^{k-1})\bw^{k - 1}, \quad \gamma \in [0, 0.5].
	\]

	\section*{Нелинейная регрессия}
	Предположим, что модель $f (\bx , \bw)$ близка к линейной в окрестности точки $\bw + \Delta \bw$
	\[
	\mathbf{f}(\bX , \bw + \Delta \bw) \approx \mathbf{f}(\bX , \bw) + \bJ \cdot \Delta  \bw,
	\]
	где $\mathbf{J} \in \bbR^{m \times p}$ является матрицы Якоби
	\begin{equation}
	\bJ = 
	\begin{pmatrix}
	\frac{\partial f(\bx_1 , \bw)}{\partial w_1} & \dots & 
	\frac{\partial f(\bx_1 , \bw)}{\partial w_p} \\
	\dots & \dots & \dots \\
	\frac{\partial f(\bx_m , \bw)}{\partial w_1} & \dots & 
	\frac{\partial f(\bx_m , \bw)}{\partial w_p}
	\end{pmatrix}.
	\end{equation}
	В соответствии с этим предположением градиент~$\nabla S(\bw)$ и Гессиан матрицы~$\bH$ функции ошибки~\eqref{eq:squared_error} равняются
	\begin{equation}
	\nabla S(\bw) = \bJ^{\T} (\by - \mathbf{f}), \quad \bH = \bJ^{\T} \bJ.
	\label{eq:nonlin_reg_deriv}
	\end{equation}
	Это приводит к методу Гаусса-Ньютона и правилу обновления~\eqref{eq:update_rule}
	\[
	\bw^k = \bw^{k - 1} + (\bJ^{\T} \bJ)^{-1}\bJ^{\T}(\mathbf{f} - \by).
	\]
	Вектор обновления~$\Delta \bw$ является решением задачи линейной регрессии
	\begin{equation}
	\| \bz - \bF \Delta \bw \|_2^2 \rightarrow \min_{\Delta \bw \in \bbR^{p}},
	\label{eq:lin_reg_nonlin_reg}
	\end{equation}
	где $\bz = \mathbf{f} - \by$ и $\bF = \bJ$.
	
	В качестве нелинейной модели рассматривается модель двухслойной нейронной сеть. В этом случае модель~$f (\bx, \bw)$ задается
	следующим образом:
	\[
	f(\bx, \bw) = \sigma(\bx^{\T} \bW_1) \bw_2.
	\]
	Здесь~$\bW_1 \in \bbR^{N \times h}$~-- это матрица весов, которые соединяют исходные признаки с $h$ скрытыми нейронами. Функция нелинейности $\sigma(\cdot)$ применяется поэлементно. Веса~$\bw_2 \in \bbR^{h \times 1}$ соединяют скрытые нейроны с выходом. 
	Вектор параметров модели~$\bw$ представляет собой объединение векторизованных матриц~$\bW_1$, $\bw_2$.

	\section*{Логистическая регрессия}
	Для логистической регрессии модель имеет вид $f(\bx , \bw) = \sigma(\bx^{\T} \bw)$ с сигмоидной функцией активации~$\sigma(\cdot)$.
	Градиент и Гессиан функции ошибки~\eqref{eq:log_loss} равны
	\begin{equation}
	\nabla S(\bw) = \bX^{\T} (\mathbf{f} - \by), \quad \bH = \bX^{\T} \bR \bX,
	\label{eq:log_reg_deriv}
	\end{equation}
	где $\bR$~-- это диагональная матрица с диагональными элементами $f(\bx_i , \bw) \cdot (1 - f(\bx_i , \bw))$.
	
	Правило обновления~\eqref{eq:update_rule} в этом случае
	\[
	\bw^k = \bw^{k - 1} + (\bX^{\T} \bR \bX)^{-1} \bX^{\T} (\by - \mathbf{f}).
	\]
	Этот алгоритм известен как итеративный алгоритм взвешенных наименьших квадратов (IRLS). Вектор обновлений $\Delta \bw$ является решением задачи линейной регрессии
	\begin{equation}
	\| \bz - \bF \Delta \bw \|_2^2 \rightarrow \min_{\Delta \bw \in \bbR^{p}},
	\label{eq:lin_reg_log_reg}
	\end{equation}
	где $\bz = \bR^{-1/2} (\by - \mathbf{f})$ и $\bF = \bR^{1/2}\bX$.

	\section*{Алгоритм}
	
	Предлагается реализовать алгоритм QPFS для решения задач~\eqref{eq:lin_reg_nonlin_reg} и \eqref{eq:lin_reg_log_reg}. 
	QPFS матрица~$\bQ$ и вектор~$\bb$ имеют вид
	\[
	\bQ = \text{Sim} (\bF), \quad \bb = \text{Rel} (\bF, \bz).
	\]
	
	Выборочный коэффициент корреляции равен нулю для ортогональных векторов.
	Покажем, что в оптимальной точке~$\bw^*$ вектор~$\bz$ ортогонален столбцам матрицы~$\bF$. 
	В этом случае вектор~$\bb = \text{Rel} (\bF, \bz)$ равен нулю. Это означает, что член, учитывающий релевантность, в данном случае исключается.
	Условие оптимизации первого порядка гарантирует это свойство для модели нелинейной регрессии
	\[
	\bF^{\T} \bz = \bJ^{\T} (\mathbf{f} - \by) = - \nabla S(\bw^*) = \boldsymbol{0},
	\]
	и для модели логистической регрессии
	\[
	\bF^{\T} \bz = \bX \bR^{-1/2} \bR^{1/2} (\by - \mathbf{f}) = \bX^{\T} (\by - \mathbf{f}) = \nabla S(\bw^*) = \boldsymbol{0}.
	\]
	Псевдокод предлагаемого алгоритма приведён в алгоритме~\ref{pc:QPFSNewton}.
	
	\begin{algorithm}
		\caption{QPFS + Ньютон алгоритм}
		\label{pc:QPFSNewton}
		\begin{algorithmic}
			\REQUIRE $\varepsilon$~-- допустимое отклонение;\\
			\hspace{1.07cm}$\tau$~-- пороговое значение;\\
			\hspace{1.07cm}$\gamma$~-- параметр правила Армихо.
			\ENSURE $\bw^*$;
			\STATE  инициализировать $\bw^0$;
			\STATE $k := 1$;
			\REPEAT
			\STATE вычислить $\bz$ и $\bF$ для~\eqref{eq:lin_reg_nonlin_reg} или~\eqref{eq:lin_reg_log_reg} ;
			\vspace{0.1cm}
			\STATE $\bQ := \text{Sim} (\bF)$, $\bb := \text{Rel}(\bF, \bz)$, $\alpha = \frac{\overline{\bQ}}{\overline{\bQ} + \overline{\bb}}$;
			\vspace{0.1cm}
			\STATE $\ba := \argmin_{\ba \geq 0, \, \|\ba\|_1=1}\ba^{\T} \bQ \ba - \alpha \cdot \mathbf{b}^{\T} \ba$;
			\vspace{0.1cm}
			\STATE $\cA = \{j \mid a_j > \tau\}$;
			\vspace{0.1cm}
			\STATE вычислить $\nabla S(\bw^{k-1})$, $\bH$ для \eqref{eq:nonlin_reg_deriv} или \eqref{eq:log_reg_deriv};
			\vspace{0.1cm}
			\STATE $\Delta \bw^{k-1} = - \bH^{-1} \nabla S(\bw^{k-1})$;
			\vspace{0.1cm}
			\STATE $\eta := \text{ArmijoRule}(\bw^{k-1}, \gamma)$;
			\vspace{0.1cm}
			\STATE $\bw_{\cA}^k = \bw_{\cA}^{k - 1} + \eta \Delta \bw_{\cA}^{k - 1}$;
			\vspace{0.1cm}
			\STATE $k := k + 1$;
			\vspace{0.1cm}
			\UNTIL{$\frac{\| \bw^k - \bw^{k-1} \|}{\| \bw^k \|} < \varepsilon$}
		\end{algorithmic}
	\end{algorithm}
	

  	\section*{Эксперимент}
  	Целью вычислительного эксперимента является исследование свойств предложенного алгоритма и сравнение его с другими методами. 
  	
  	Исследована зависимость параметров алгоритма QPFS для задач~\eqref{eq:lin_reg_nonlin_reg},~\eqref{eq:lin_reg_log_reg}. 
  	Предположим, что вектор параметров~$\bw^0$ лежит вблизи оптимального вектора параметров~$\bw^*$. 
  	Рассмотрим отрезок
	\[
	\bw_{\beta} = \beta \bw^* + (1 - \beta) \bw^0; \, \beta \in [0, 1] .
	\]
	
	Сгенерируем синтетический набор данных с 300 объектами и 7 признаками для задачи логистической регрессии. 
	Ландшафт функции ошибки~\eqref{eq:log_loss} на сетке двух случайно выбранных параметров показан на рис.~\ref{fig:log_reg_error}.
	Поверхность функции ошибки выпуклая с вытянутыми линиями уровня вдоль некоторых параметров модели.
	Добавим случайный шум к оптимальным параметрам~$\bw^*$, чтобы получить точку~$\bw^0$. Поведение вектора~$\bb$ на отрезке между~$\bw^0$ и~$\bw^*$ показано на рис.~\ref{fig:log_reg_b_wrt_beta}.
	Компоненты~$\bb$ начинают резко уменьшаться, приближаясь к оптимальной точке.
	\begin{figure}
		\centering
		\begin{minipage}{.47\textwidth}
			\centering
			\includegraphics[width=\linewidth]{figs/log_reg_error}
			\caption{Ландшафт функции ошибки для логистической регрессии}
			\label{fig:log_reg_error}
		\end{minipage}%
		\begin{minipage}{.47\textwidth}
			\centering
			\includegraphics[width=\linewidth]{figs/log_reg_b_wrt_beta}
			\caption{Релевантности параметров для логистической регрессии}
			\label{fig:log_reg_b_wrt_beta}
		\end{minipage}
	\end{figure}

	Для модели нелинейной регрессии используется классический набор данных Boston Housing с 506 объектами и 13 признаками.
	Для простоты нейронная сеть содержит два скрытых нейрона.
	Ландшафт функции ошибок для модели нейронной сети является более сложным. 
	Он не выпуклый и может содержать несколько локальных минимумов.
	Двумерный ландшафт функции ошибок для этого набора данных показан на рис.~\ref{fig:neural_error}. 
	Сетка строится для двух случайных весов из матрицы~$\bW_1$.
	Мы используем ту же стратегию для исследования того, как вектор~$\bb$ изменяется от~$\bw^0$ до~$\bw^*$. 
	Результат показан на рис.~\ref{fig:neural_b_wrt_beta}.
	Компоненты вектора $\bb$ становятся близки к нулю вблизи оптимума. 
	При достижении оптимального значения различные веса влияют на остатки модели~$\bz$.
	\begin{figure}
		\centering
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=\linewidth]{figs/neural_error}
			\caption{Ландшафт функции ошибки для нейронной сети}
			\label{fig:neural_error}
		\end{minipage}%
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=\linewidth]{figs/neural_b_wrt_beta}
			\caption{Релевантности параметров первого слоя для модели нейронной сети}
			\label{fig:neural_b_wrt_beta}
		\end{minipage}
	\end{figure}

	На рис.~\ref{fig:irls_qpfs_2d} показан процесс оптимизации для предложенного алгоритма в случае логистической регрессии с двумя параметрами модели. 
	Даже для двумерной задачи решение метода Ньютона нестабильно и число обусловленности матрицы Гессиана~$\bH$ может быть чрезвычайно большим. 
	На каждом шаге алгоритма процедура QPFS выбирает параметры для оптимизации. 
	В данном примере предложенный алгоритм выбирает и обновляет только один параметр на каждой итерации на первых шагах. 
	Это делает алгоритм более устойчивым.

	\begin{figure}[!h]
		\centering
		\includegraphics[width=0.6\linewidth]{figs/irls_qpfs_2d}	 
		\caption{Оптимизационный процесс предложенного алгоритма QPFS+Ньютон для модели логистической регрессии}
		\label{fig:irls_qpfs_2d}
	\end{figure}
	
	На рис.~\ref{fig:active_params_wrt_iters} показаны наборы активных параметров на итерациях для набора данных Boston Housing и нейронной сети с двумя скрытыми нейронами. 
	Темные ячейки соответствуют активным параметрам, которые мы оптимизируем.
	
	\begin{figure}[!h]
		\centering
		\includegraphics[width=\linewidth]{figs/active_params_wrt_iters}	
		\caption{Множества активных параметров на протяжении оптимизационного процесса}
		\label{fig:active_params_wrt_iters}
	\end{figure}
	
	В рассмотренных примерах число обусловленности~$\kappa(\bH)$ для метода Ньютона на некоторых итерациях было чрезвычайно большим. 
	Выбор активных параметров позволил значительно сократить число обусловленности. 
	
	Мы сравнили предложенный алгоритм с существующими методами, а именно градиентным спуском~(GD), моментом Нестерова, Adam и оригинальным алгоритмом Ньютона. 
	Проведены эксперименты для моделей нелинейной и логистической регрессий. 
	Наборы данных были выбраны из репозитория UCI~\cite{uci2017}. 
	Результаты показаны в таблицах~\ref{tbl:nonlin_reg_results} и \ref{tbl:log_reg_results}. 
	Для каждого набора данных две строки содержат ошибки для тренировочной~(первая строка) и тестовой~(вторая строка) выборок. 
	В таблице~\ref{tbl:nonlin_reg_results} приведена квадратичная ошибка, в таблице~\ref{tbl:log_reg_results}~-- кросс-энтропия.
	Чтобы найти среднюю ошибку и ее стандартное отклонение использовалась процедура кросс валидации на 5 фолдов. 
	Предложенный алгоритм показывает меньшую ошибку на трех из четырех наборов данных для нелинейной регрессии и среди двух из трех наборов данных для логистической регрессии.
	
	\begin{table}[!h]
	\footnotesize
		\caption{Средняя квадратичная ошибка на тренировочной и тестовой выборках для модели нелинейной регрессии}
		\label{tbl:nonlin_reg_results}
		\centering
		\begin{tabular}{|l|c|c|c|c|c|c|}
			\hline
			Выборка & \begin{tabular}[c]{@{}c@{}}\ $m$ \\ \ $n$ \end{tabular} 
			& GD 
			& Нестеров 
			& ADAM 
			& Ньютон 
			&
			\begin{tabular}[c]{@{}c@{}}QPFS+Ньютон\\ \end{tabular} \\ 
			\hline
			Boston House
			& 506
			& $27.2\pm4.6$
			& $46.0\pm11.0$
			& $35.4\pm2.5$           
			& $22.1\pm15.2$            
			& $20.9\pm10.4$   \\  
			Prices
			&\multicolumn{1}{c|}{13}
			& \multicolumn{1}{c|}{$32.4\pm5.6$}
			& \multicolumn{1}{c|}{$53.3\pm11.5$}
			& \multicolumn{1}{c|}{$37.8\pm7.0$}
			& \multicolumn{1}{c|}{$28.9\pm13.6$}
			& \multicolumn{1}{c|}{$\mathbf{24.5\pm9.4}$}\\ 
			\hline
			Communities
			& 1994
			& $48.0\pm6.4$
			& $31.4\pm2.8$
			& $23.3\pm3.7$        
			& $18.3\pm3.4$          
			&  $26.7\pm3.1$  \\ 
			and Crime
			&\multicolumn{1}{c|}{99}
			& \multicolumn{1}{c|}{$47,5\pm6.5$}
			& \multicolumn{1}{c|}{$32.9\pm4.3$}
			& \multicolumn{1}{c|}{$28,1\pm4.5$}
			& \multicolumn{1}{c|}{$28.8\pm3.6$}
			& \multicolumn{1}{c|}{$\mathbf{28.4\pm3.0}$} \\ 
			\hline
			Forest
			& 517
			& $18.9\pm0.4$
			& $1.83\pm0.4$
			& $1.81\pm0.6$             
			& $17.7\pm0.4$             
			& $17.9\pm0.4$   \\ 
			Fires
			&\multicolumn{1}{c|}{10}
			& \multicolumn{1}{c|}{$\mathbf{20.0\pm2.1}$}
			& \multicolumn{1}{c|}{ $20.2\pm2.2$}
			& \multicolumn{1}{c|}{ $\mathbf{20.0\pm2.0}$}
			& \multicolumn{1}{c|}{ $20.6\pm1.4$}
			& \multicolumn{1}{c|}{ $20.2\pm2.2$} \\ 
			\hline
			Residential
			& 372
			&  $51.6\pm17.7$
			&  $32.6\pm19.5$
			&  $30.0\pm24.8$            
			&  $35.5\pm24.7$            
			&   $30.3\pm10.7$ \\ 
			Building
			&\multicolumn{1}{c|}{103}
			& \multicolumn{1}{c|}{ $53.7\pm13.9$}
			& \multicolumn{1}{c|}{ $34.1\pm13.6$}
			& \multicolumn{1}{c|}{ $34.1\pm19.4$}
			& \multicolumn{1}{c|}{ $35.0\pm15.6$}
			& \multicolumn{1}{c|}{ $\mathbf{30.9\pm5.3}$} \\ 
			\hline
		\end{tabular}
	\end{table}
	
	\begin{table}[!h]
	\footnotesize
		\caption{Среднее значение кросс-энтропии на тренировочной и тестовой выборках для модели логистической регрессии}
		\label{tbl:log_reg_results}
		\centering
		\begin{tabular}{|l|c|c|c|c|c|c|}
			\hline
			Выборка & \begin{tabular}[c]{@{}c@{}}\ $m$ \\ \ $n$ \end{tabular} 
			& GD 
			& Нестеров 
			& ADAM 
			& Ньютон 
			&
			\begin{tabular}[c]{@{}c@{}}QPFS+Ньютон\\ \end{tabular} \\ 
			\hline
			Breast
			& 569
			& $0.6\pm0.1$
			& $0.4\pm0.1$
			& $0.8\pm0.2$           
			& $0.3\pm0.1$            
			& $0.2\pm0.1$   \\  
			Cancer
			&\multicolumn{1}{c|}{30}
			& \multicolumn{1}{c|}{$\mathbf{0.9\pm0.2}$}
			& \multicolumn{1}{c|}{$1.0\pm0.7$}
			& \multicolumn{1}{c|}{$1.2\pm0.2$}
			& \multicolumn{1}{c|}{$1.0\pm0.2$}
			& \multicolumn{1}{c|}{$1.1\pm0.3$}\\ 
			\hline
			Cardiotocography
			& 2126
			& $11.5\pm4.7$
			& $11.5\pm4.7$
			& $8.8\pm4.4$        
			& $11.5\pm5.7$          
			&  $7.7\pm4.2$  \\
			
			&\multicolumn{1}{c|}{21}
			& \multicolumn{1}{c|}{$11.6\pm5.8$}
			& \multicolumn{1}{c|}{$11.5\pm5.7$}
			& \multicolumn{1}{c|}{$9.0\pm2.6$}
			& \multicolumn{1}{c|}{$11.5\pm4.7$}
			& \multicolumn{1}{c|}{$\mathbf{7.7\pm4.7}$} \\ 
			\hline
			Climate Model
			& 540
			& $1.2\pm0.1$
			& $1.0\pm0.2$
			& $1.5\pm0.2$             
			& $1.0\pm0.5$             
			& $0.8\pm0.3$   \\ 
			Simulation Crashes
			&\multicolumn{1}{c|}{18}
			& \multicolumn{1}{c|}{$1.4\pm2.0$}
			& \multicolumn{1}{c|}{ $1.3\pm0.7$}
			& \multicolumn{1}{c|}{ $1.8\pm0.3$}
			& \multicolumn{1}{c|}{ $1.2\pm0.5$}
			& \multicolumn{1}{c|}{ $\mathbf{1.1\pm0.4}$} \\ 
			\hline
		\end{tabular}
	\end{table}
	\section*{Заключение}
	В статье решается задача устойчивой оптимизации для прогностической модели.
	Предложен новый алгоритм второго порядка для оптимизации. 
	Выбор набора активных параметров модели позволяет делать градиентные шаги в направлениях, более соответствующим остаткам.
	Рассмотрены модели нелинейной и логистической регрессий.
	Эксперименты показывают, что предложенный алгоритм улучшает обобщающую способность и снижает ошибку модели.

  \bibliographystyle{unsrt}
  \bibliography{papers_qpfs.bib}
	\end{document}