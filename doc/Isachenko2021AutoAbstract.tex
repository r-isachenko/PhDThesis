\documentclass[11pt, a5paper]{dissert}

\usepackage{fullpage}
\usepackage{microtype}
\renewcommand{\baselinestretch}{1.0}

\input{preamble.tex}
\mathtoolsset{showonlyrefs=true} 
\input{newcommands.tex}

\begin{document}

\begin{titlepage}
	\begin{flushright}
		{На правах рукописи}
	\end{flushright}
	\vspace{3.5cm}
	\begin{center}
		{Исаченко Роман Владимирович}
		\par
		\vspace{3cm}
		\textsc{Снижение размерности пространства \\ в задачах декодирования сигналов}
		\par
		\vspace{2cm}
		{05.13.17~--- Теоретические основы информатики}
		\par
		\vspace{2cm}
		{АВТОРЕФЕРАТ\\
		диссертации на соискание ученой степени\\
		кандидата физико-математических наук}
	\end{center}
	\par
	\vspace{3.5cm}
	\begin{center}
		{Москва~--- 2021}
	\end{center}
\end{titlepage}


\setcounter{page}{2}

\noindent {Работа выполнена на Кафедре интеллектуальных систем Федерального государственного автономного образовательного учреждения высшего образования <<Московский физико-технический институт (национальный исследовательский университет)>>.

\vspace{0.1cm}

\vskip1ex\noindent
\begin{tabularx}{\linewidth}{@{}lX@{}}
  Научный руководитель: & \textbf{Стрижов Вадим Викторович}\\
  & доктор физико-математических наук, Федеральный исследовательский~центр <<Информатика и управление>> Российской академии наук, отдел интеллектуальных систем, ведущий научный сотрудник.
  \\[2pt]
  Официальные оппоненты: & \textbf{\color{red}Чуличков Алексей Иванович}\\
  & \textcolor{red}{доктор физико-математических наук, профессор, Федеральное государственное бюджетное образовательное учреждение высшего образования <<Московский государственный университет имени М. В.~Ломоносова>>, профессор кафедры математического моделирования и информатики физического факультета.}\\[2pt]
  & \textbf{\color{red}Зайцев Алексей Алексеевич}\\
  & \textcolor{red}{кандидат физико-математических наук, Автономная некоммерческая
образовательная организация высшего образования <<Сколковский институт науки и технологий>>, руководитель лаборатории в Центре по научным и инженерным вычислительным технологиям для задач с большими массивами данных.} \\[2pt]
  Ведущая организация: & \textcolor{red}{Федеральное государственное автономное образовательное учреждение высшего образования <<Санкт-Петербургский национальный исследовательский университет информационных технологий, механики и оптики>>.}
\end{tabularx}
\vskip2ex\noindent


\vspace{0.2cm}
\noindent Защита состоится{\color{red}~6~февраля 2020 года~в~13:00} на~заседании диссертационного совета Д 002.073.05 при Федеральном исследовательском центре <<Информатика и управление>> Российской академии наук (ФИЦ~ИУ~РАН) по адресу: 119333, г.\,Москва, ул.\,Вавилова, д.\,40.

\vspace{0.2cm}
\noindent С диссертацией можно ознакомиться в библиотеке Федерального государственного учреждения Федеральный исследовательский центр <<Информатика и управление>> Российской академии наук и на сайте http://www.frccsc.ru/

\vspace{0.2cm}
\noindent Автореферат разослан  \quad \quad \textcolor{red}{декабря 2021 года.}

\vspace{0.3cm}
\noindent И. о. ученого секретаря\\
диссертационного совета Д 002.073.05\\
д.т.н.
\hspace{12cm} И. А. Матвеев
}

\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pretolerance=-1

\section*{Общая характеристика работы}
\label{ch:Intro}

\textbf{Актуальность темы.} 
В работе исследуется проблема снижения размерности пространства при решении задачи декодирования сигналов. 
Процесс декодирования заключается в восстановлении зависимости между двумя гетерогенными наборами данных.
Прогностическая модель предсказывает отклик на входной исходный сигнал.

Исходное описание данных является избыточным. 
При высокой мультикорреляции в признаковом пространстве финальная прогностическая модель оказывается неустойчивой.
Для построения простой, устойчивой модели применяются методы снижения размерности пространства~(Motrenko:~2018, Chun:~2010, Mehmood:~2012)  и выбора признаков~(Katrutsa:~2017, Li:~2017).

В работе решается задача декодирования с векторной целевой переменной. 
Пространство целевых сигналов обладает избыточной размерностью. 
Методы снижения размерности, не учитывающие зависимости в целевом пространстве, не являются адекватными.
При предсказании векторной целевой переменной анализируется структура целевого пространства.
Предложены методы, которые учитывают зависимости как в пространстве исходных объектов, так и в пространстве целевой переменной.
Предлагается отобразить пространства исходных и целевых сигналов в скрытые подпространства меньшей размерности.
Для построения оптимальной модели предлагаются методы согласования скрытых пространств~(Wold:~1975, Rosipal:~2005, Eliseyev:~2017).
Предложенные методы позволяют учесть регрессионную компоненту между исходным и целевым сигналами, а также авторегрессионную компоненту целевого сигнала.

Методы снижения размерности пространства понижают размерность исходного пространства объектов, и, как следствие, сложность модели существенно снижается~(Tipping:~1999, Hotelling:~1992). 
Алгоритмы снижения размерности находят оптимальные комбинации исходных признаков. 
Если число таких комбинаций существенно меньше, чем число исходных признаков, то полученное представление снижает размерность.
Цель снижения размерности~--- получение наиболее репрезентативных и информативных комбинаций признаков для решения задачи.

Выбор признаков является частным случаем снижения размерности пространства~(Katrutsa: 2015, 2017). 
Найденные комбинации признаков являются подмножеством исходных признаков.
Таким образом отсеиваются шумовые неинформативные признаки.
Рассматриваются два типа методов выбора признаков~(Li:~2017, Rodriguez-Lujan:~2010, Friedman:~2001).
Первый тип методов не зависит от последующей прогностической модели.
Признаки отбираются на основе свойств исходных пространств, а не на основе свойств модели.
Второй тип методов отбирает признаки с учётом знания о прогностической модели. 

После нахождения оптимального представления данных с помощью снижения размерности, ставится задача нахождения оптимальной метрики в скрытом пространстве объектов~(Wang:~2017, Davis:~2007, Kulis: 2012, Yang:~2006, Weinberger:~2009).
В случае евклидова пространства естественным выбором метрики оказывается квадратичная норма.
Задача метрического обучения заключается в нахождении оптимальной метрики, связывающей объекты.

В качестве прикладной задачи анализируется задача построения нейрокомпьютерного интерфейса~(Wolpaw:~2000, Allison:~2007). 
Цель состоит в извлечении информации из сигналов мозговой активности~(Nagel:~2018, Zhang:~2020, Chiarelli:~2018). 
В качестве исходных сигналов выступают сигналы электроэнцефалограммы или электрокортикограммы. 
Целевым сигналом является траектория движения конечности индивидуума.
Необходимо построить адекватную и эффективную модель декодирования исходного сигнала в целевой сигнал.
Пространство частотных характеристик мозговых сигналов и авторегрессионное пространство целевых сигналов являются чрезвычайно избыточными~(Eliseyev:~2011, 2013). 
Построение модели без учёта имеющихся зависимостей приводит к неустойчивости модели.

В диссертации решается задача декодирования с векторной целевой переменной. 
Для построения оптимальной модели декодирования сигналов предлагаются методы выбора согласованных моделей с проекцией в скрытое пространство.
Исходные и целевые сигналы проецируются в пространство существенно меньшей размерности. 
Для связи проекций исходного и целевого сигналов предлагаются методы согласования.
Рассматриваются гетерогенные наборы сигналов, природа источников измерений различны.
Рассматриваются как линейные методы декодирования, так и их нелинейные обобщения.
Доказаны теоремы об оптимальности предложенных методов выбора моделей.

\vspace{0.5cm}
\textbf{Цели работы.}
\begin{enumerate}
	\item Исследовать свойства решения задачи декодирования сигналов с векторной целевой переменной.
	\item Предложить методы снижения размерности пространства, учитывающие зависимости как в пространстве исходных сигналов, так и в целевом пространстве.
	\item Предложить процедуру выбора признаков для задачи декодирования сигналов.
	\item Исследовать свойства линейных и нелинейных моделей для решения поставленной модели. Получить теоретические оценки оптимальности моделей.
	\item Провести вычислительные эксперименты для проверки адекватности предложенных методов.
\end{enumerate}


\vspace{0.5cm}
\textbf{Основные положения, выносимые на защиту.}
\begin{enumerate}
	\item Исследована проблема снижения размерности сигналов в коррелированных пространствах высокой размерности. Предложены методы декодирования сигналов, учитывающие зависимости как в исходном, так и в целевом пространстве сигналов.
	\item Доказаны теоремы об оптимальности предлагаемых методов декодирования сигналов. Предлагаемые методы выбирают согласованные модели в случае избыточной размерности описания данных.
	\item Предложены методы выбора признаков, учитывающие зависимости как в исходном, так и в целевом пространстве. Предложенные методы доставляют устойчивые и адекватные решения в пространствах высокой размерности. 
	\item Предложены нелинейные методы согласования скрытых пространств для данных со сложноорганизованной целевой переменной. Предложен метод выбора активных параметров для оптимизации нелинейной модели. Исследованы свойства предлагаемого метода.
	\item Предложен алгоритм метрического обучения для временных рядов с процедурой их выравнивания.
	\item Предложен ряд моделей для прогнозирования гетерогенных наборов сигналов для задачи построения нейрокомпьютерных интерфейсов. Проведены вычислительные эксперименты, подтверждающие адекватность моделей.
\end{enumerate}

\vspace{0.5cm}
\textbf{Методы исследования.}
Для достижения поставленных целей используются линейные и нелинейные методы регрессионного анализа.
Для анализа временных рядов используются классические авторегрессионные методы.
Для извлечения признаков используются частотные характеристики временного ряда.
Для построения скрытого пространства используются линейные методы снижения размерности пространства, их нелинейные модификации, а также нейросетевые методы.
Для выбора признаков наряду с классическими методами, используются методы, основанные на решении задачи квадратичного программирования.
Для построения метрического пространства используются методы условной выпуклой оптимизации.

\vspace{0.5cm}
\textbf{Научная новизна.}
Предложены методы построения моделей декодирования сигналов, учитывающие структуры пространств исходных и целевых переменных.
Предложены методы проекции сигналов в скрытое пространство, а также процедуры согласования образов.
Предложены методы выбора признаков с помощью квадратичного программирования.
Предложен метод выбора активных параметров нелинейной модели с помощью выбора признаков.
Предложены методы построения оптимального метрического пространства для задачи анализа временных рядов.

\vspace{0.5cm}
\textbf{Теоретическая значимость.}
Доказаны теоремы об оптимальности предлагаемых моделей декодирования сигналов.
Доказаны теоремы о корректности рассматриваемых согласованных моделей проекций в скрытое пространство.
Доказаны теоремы о достижении точки равновесия для предлагаемых методов выбора признаков. 

\vspace{0.5cm}
\textbf{Практическая значимость.}
Предложенные в работе методы предназначены для декодирования множества временных рядов сигналов электрокортикограмм, а также нестационарных временных рядов; выбора оптимальных частотных характеристик сигналов; выбора наиболее информативных параметров модели; классификации и кластеризации временных рядов физической активности.

\vspace{0.5cm}
\textbf{Степень достоверности и апробация работы.}
Достоверность результатов подтверждена математическими доказательствами, экспериментальной проверкой результатов предлагаемых методов на реальных данных, публикациями результатов в рецензируемых научных изданиях, в том числе рекомендованных ВАК. 
Результаты работы докладывались и обсуждались на следующих научных конференциях.
\vspace{0.2cm}
\begin{enumerate}
	\item Р. В. Исаченко. Метрическое обучение в задачах мультиклассовой классификации временных рядов. \textit{Международная научная конференция <<Ломоносов>>}, 2016.
	\item R. G. Neychev, A. P. Motrenko, R. V. Isachenko, A. S. Inyakin, and V. V. Strijov. Multimodal forecasting multiscale time series in internet of things. \textit{Международная научная конференция  <<11th International Conference on Intelligent Data Processing: Theory and Applications>>}, 2016.
	\item Р. В. Исаченко, И. Н. Жариков, и А. М. Бочкарёв. Локальные модели для классификации объектов сложной структуры. \textit{Всероссийская научная конференция <<Математические методы распознавания образов>>}, 2017.
	\item R. V. Isachenko and V. V. Strijov. Dimensionality reduction for multicorrelated signal decoding with projections to latent space. \textit{Международная научная конференция  <<12th International Conference on Intelligent Data Processing: Theory and Applications>>}, 2018.
	\item Р. В. Исаченко, В. В. Стрижов. Снижение размерности в задаче декодирования временных рядов. \textit{Международная научная конференция  <<13th International Conference on Intelligent Data Processing: Theory and Applications>>}, 2020.
\end{enumerate} 

\vspace{0.5cm}
\textbf{Публикации по теме диссертации.}
Основные результаты по теме диссертации изложены в 6 печатных изданиях, 5 из которых изданы в журналах, рекомендованных ВАК.

\vspace{0.5cm}
\textbf{Структура и объем работы.}
Диссертация состоит из оглавления, введения, 6 глав, заключения, списка иллюстраций, списка таблиц, списка основных обозначений и списка литературы из~110 наименований. 
Основной текст занимает~121 страниц.

\vspace{0.5cm}
\textbf{Личный вклад.}
Все приведенные результаты, кроме отдельно оговоренных случаев, получены диссертантом лично при научном руководстве д.ф.-м.н. В.\,В.~Стрижова.

\section*{Основное содержание работы}

Во \textbf{введении} обоснована актуальность диссертационной работы, сформулированы цели и методы исследования, обоснована научная новизна, теоретическая и практическая значимости полученных результатов.

В \textbf{главе 1} ставится общая задача декодирования временных рядов.
Ставится задача построения оптимальной линейной регрессионной модели декодирования.

Пусть $\bbX \subset \bbR^n$~--- пространство исходной переменной, $\bbY \subset \bbR^r$~--- пространство целевой переменной.
Пусть задано множество объектов $\{(\bx_i, \by_i)\}_{i=1}^m$, где $\bx_i \in \bbX$~--- исходный объект, $\by_i \in \bbY$~--- целевой объект.

Обозначим за $\bX \in \bbR^{m \times n}$ матрицу исходной переменной, за $\bY \in \bbR^{n \times k}$ матрицу целевой переменной:
\begin{equation*}
	\bX = [\bx_1, \dots, \bx_m]^{\T} =  [\bchi_1, \dots, \bchi_n]; \quad \bY = [\by_1, \dots, \by_m]^{\T} =  [\bnu_1, \dots, \bnu_r].
\end{equation*}

Столбцы~$\{\bchi_j\}_{j=1}^n$ матрицы~$\bX$ являются исходными признаками, столбцы~$\{\bnu_j\}_{j=1}^r$ матрицы~$\bY$ являются целевыми векторами.

Предполагается, что между исходным объектом $\bx$ и целевым объектом $\by$ существует зависимость. Требуется построить прогностическую модель $\mathbf{f}: \bbX \rightarrow \bbY$.

Задача восстановления регрессионной зависимости состоит в нахождении оптимальной модели $\mathbf{f}^*$ по заданным матрицам $\bX$ и $\bY$:
\begin{equation}
	\mathbf{f}^* = \argmin_\mathbf{f} \cL(\mathbf{f}, \bX, \bY).
	\label{ch1:eq:loss_min}
\end{equation}

Для сужения пространства поиска моделей будем рассматривать параметрические модели $\mathbf{f}(\bx, \bTheta)$, где $\bTheta$~--- \textit{параметры модели}. 
Таким образом между объектами $\bx$ и $\by$ существует зависимость вида
\begin{equation}
	\by = \mathbf{f}(\bx, \bTheta) + \boldsymbol{\varepsilon},
	\label{ch1:eq:reg_model}
\end{equation}
где $\mathbf{f}$~--- параметрическая прогностическая модель, $\bTheta$~--- параметры модели, $\bepsilon \in \bbR^{m}$~--- вектор регрессионных остатков. 

Задача~\eqref{ch1:eq:loss_min} сводится к задаче поиска оптимальных параметров
\begin{equation}
	\bTheta^* = \argmin_{\bTheta} \cL(\bTheta, \bX, \bY).
	\label{ch1:eq:loss_min_param}
\end{equation}

Предположим, что зависимость $\mathbf{f}(\bx, \bTheta)$ линейная:
\begin{equation}
	\by = \mathbf{f}(\bx, \bTheta) + \bepsilon = \bTheta^{\T} \bx+ \bepsilon,
	\label{ch1:eq:lin_reg_model}
\end{equation}
где $\bTheta \in \bbR^{n \times r}$~--- матрица параметров модели.

Оптимальные параметры~$\bTheta$ определяются минимизацией функции ошибки:
\begin{equation}
	\cL(\bTheta, \bX, \bY) = {\left\| \underset{m \times r}{\mathbf{Y}}  - \underset{m \times n}{\bX} \cdot \underset{r \times n}{\bTheta} \right\| }_2^2 \rightarrow\min_{\bTheta}.
	\label{ch1:eq:l2_loss_function}
\end{equation}
Решением~\eqref{ch1:eq:l2_loss_function} является следующая матрица:
\begin{equation*}
	\bTheta = (\bX^{\T} \bX)^{-1} \bX^{\T} \bY.
\end{equation*}

Если существует вектор~$\boldsymbol{\alpha} \neq \bZero_n$ такой, что $\bX \boldsymbol{\alpha}= \bZero_m$, то добавление~$\boldsymbol{\alpha}$ к любому столбцу матрицы~$\bTheta$ не меняет значение функции потерь~$\cL(\bTheta, \bX, \bY)$.
В этом случае матрица~$\bX^{\T} \bX$ близка к сингулярной и не обратима.

Задача декодирования сигналов состоит в восстановлении регрессионной зависимости~\eqref{ch1:eq:loss_min} между наборами гетерогенных сигналов.

Пусть имеется два множества временных рядов $\mathcal{S}_{\bx} = \{\bs_{\bx}^i\}_{i=1}^m$ и $\mathcal{S}_{\by} = \{\bs_{\by}^i\}_{i=1}^r$, состоящие из $m$ и $r$ временных рядов соответственно. 
Первое множество $\mathcal{S}_{\bx}$ является множеством временных рядов $m$ исходных сигналов. 
Второе множество~$\mathcal{S}_{\by}$ является множеством временных рядов $r$ целевых сигналов.
Каждый временной ряд $\bs = (s_1, s_2, \dots, s_T)$ является последовательностью измерений некоторый величины в течение времени. 
\begin{definition}
	 \textit{Временное представление} $\bx_t = ([\bs_{\bx}^1]_t, \dots, [\bs_{\bx}^m]_t) \in \bbR^m$ состоит из измерений временных рядов исходных сигналов в момент времени $t$. 
	Аналогично временное представление $\by_t = ([\bs_{\by}^1]_t, \dots, [\bs_{\by}^r]_t) \in \bbR^r$ состоит из измерений временных рядов целевых сигналов в момент времени $t$.
\end{definition}
\begin{definition}
	Определим \textit{представление предыстории} длины $h$ для момента времени $t$ множества временных рядов исходных сигналов $\mathcal{S}_{\bx}$ как совокупность представлений $\bX_{t,h} = [\bx_{t - h + 1}, \dots, \bx_{t}]^{\T} \in \bbR^{h \times m}$.
	Аналогично определим представление предыстории длины $h$ для момента времени $t$ множества временных рядов целевых сигналов $\mathcal{S}_{\by}$ как совокупность представлений $\bY_{t,h} = [\by_{t - h + 1}, \dots, \by_{t}]^{\T} \in \bbR^{h \times r}$.
\end{definition}
\begin{definition}
	Определим \textit{представление горизонта прогнозирования} длины $p$ для момента времени $t$ множества временных рядов исходных сигналов~$\mathcal{S}_{\bx}$ как совокупность представлений $\bX_{t,p} = [\bx_{t + 1}, \dots, \bx_{t + p}]^{\T} \in \bbR^{p \times m}$.
	Аналогично определим представление горизонта прогнозирования длины $p$ для момента времени $t$ множества временных рядов целевых сигналов $\mathcal{S}_{\by}$ как совокупность представлений $\bY_{t,r} = [\by_{t + 1}, \dots, \by_{t + p}]^{\T} \in \bbR^{p \times r}$.
\end{definition}

\begin{definition}
	\label{ch1:def:autoreg_model}
	Прогностическая модель $\mathbf{f}^{\text{AR}}_{\bx}: \bbR^{h \times m} \rightarrow \bbR^{p \times m}$ является \textit{авторегрессионной моделью}, которая по представлению предыстории $\bX_{t,h}$ множества временных рядов исходных сигналов $\mathcal{S}_{\bx}$ предсказывает представление горизонта прогнозирования $\bX_{t,p}$ множества временных рядов исходных сигналов $\mathcal{S}_{\bx}$.
	Аналогично вводится прогностическая модель $\mathbf{f}^{\text{AR}}_{\by}: \bbR^{h \times r} \rightarrow \bbR^{p \times r}$ для множества целевых сигналов $\mathcal{S}_{\by}$.
\end{definition}

\begin{definition}
	\label{ch1:def:reg_model}
	Определим задачу \textit{регрессионного декодирования} как задачу построения прогностической модели $\mathbf{f}^{\text{R}}_{\bx\by}: \bbR^{h \times m} \rightarrow \bbR^{p \times r}$, которая по представлению предыстории $\bX_{t,h}$ множества временных рядов исходных сигналов $\mathcal{S}_{\bx}$ предсказывает представление горизонта прогнозирования $\bY_{t,p}$ множества временных рядов целевых сигналов $\mathcal{S}_{\by}$.
\end{definition}

\begin{definition}
	\label{ch1:def:decode_model}
	Общая \textit{задача декодирования} состоит в построении прогностической модели $\mathbf{f}_{\bx\by}: \bbR^{h_x \times m} \times \bbR^{h_y \times r} \rightarrow \bbR^{p \times r}$, которая по представлениям предыстории $\bX_{t,h_x}$ и $\bY_{t,h_y}$ временных рядов исходных и целевых сигналов предсказывает представление горизонта прогнозирования $\bY_{t,r}$ временных рядов целевых сигналов. 
\end{definition}

Задача декодирования временных рядов декомпозируется на следующие подзадачи.
\begin{itemize}
	\item Порождение признакового пространства.  
	Процедура порождения признакового пространства может быть основана на экспертных знаниях или же являться моделью машинного обучения. 
	
	\item Снижение размерности пространства или выбор признаков. 
	Исходные временные ряды, а также порожденное признаковое пространство оказывается избыточным, что приводит к избыточности и неустойчивости модели. 
	
	\item Построение модели.
	После нахождения оптимального низкоразмерного представления исходных данных ставится задача выбора оптимальной модели декодирования.
\end{itemize}

\vspace{0.5cm}
\textbf{Метод главных компонент для задачи декодирования.}
Mетод главных компонент~(PCA) находит низкоразмерное представление матрицы~$\bX = \bT \bP$, такое что новое представление~$\bT \in \bbR^{m \times l}$ содержит максимальную долю дисперсии исходной матрицы.
При этом матрица отображения $\bP \in \bbR^{l \times n}$ ($\bP \bP^{\T} = \bI$) содержит правые собственные вектора матрицы ковариаций $\bX^{\T} \bX$.

После нахождения матрицы отображения $\bP$ задача~\eqref{ch1:eq:l2_loss_function} принимает вид
\begin{equation}
	\cL(\bB, \bT, \bY) = {\left\| \underset{m \times r}{\mathbf{Y}}  - \underset{m \times l}{\bT} \cdot \underset{l \times r}{\bB} \right\| }_2^2 \rightarrow\min_{\bB}.
	\label{ch1:eq:l2_loss_function_pca}
\end{equation}

Модель прогнозирования~\eqref{ch1:eq:lin_reg_model} в случае снижения размерности с помощью PCA принимает вид:
\begin{equation}
	\by = \bB^{\T} \bt + \bepsilon = \bB^{\T} \bP \bx + \bepsilon = \bTheta \bx + \bepsilon, \, \text{ где } \bTheta = \bB^{\T} \bP.
	\label{ch1:eq:lin_reg_model_pca}
\end{equation}

\vspace{0.5cm}
\textbf{Метод частичных наименьших квадратов для задачи декодирования.}
Основным недостатком метода PCA является отсутствие учёта взаимосвязи между исходными признаками~$\bchi_j$ и целевыми векторами~$\bnu_j$.
Метод частичных наименьших квадратов (PLS) проецирует матрицу исходных объектов~$\bX$ и матрицу целевых объектов~$\bY$ в скрытое пространство малой размерностью~$l$ ($l < n$).
Метод PLS находит в скрытом пространстве матрицы~$\bT, \bU \in \bbR^{m \times l}$, которые лучше всего описывают исходные матрицы~$\bX$ и~$\bY$. 
При этом PLS максимизирует ковариацию между столбцами матриц $\bT$ и $\bU$ соответственно.
Принцип работы метода показан на следующей коммутативной диаграмме:
\begin{equation}
	\begin{tikzpicture}
		\matrix (m) [matrix of math nodes,row sep=3em,column sep=2em,minimum width=2em,ampersand replacement=\&]
		{
			\bx \in \bbR^n \& \& \by \in \bbR^r \\
			\& \bt, \bu \in \bbR^\ell \& \\};
		\path[-stealth]
		(m-1-1) edge node [above] {$\mathbf{f}$} (m-1-3)
		(m-1-1) edge [bend right=10] node [below, pos=0.4] {$\bW$} (m-2-2)
		(m-2-2) edge [bend right=10] node [above, pos=0.4] {$\bP$} (m-1-1)
		(m-1-3) edge [bend left=10] node [below, pos=0.4] {$\bC$} (m-2-2)
		(m-2-2) edge [bend left=10] node [above, pos=0.4] {$\bQ$} (m-1-3);
	\end{tikzpicture}
\end{equation}
Матрица исходных объектов $\bX$ и матрица целевых объектов $\bY$ проецируются на скрытое пространство следующим образом:
\begin{align}
	\label{ch1:eq:PLS_X}
	\underset{m \times n}{\bX} 
	&= \underset{m \times l}{\bT} \cdot \underset{l \times n}{\bP} + \underset{m \times n}{\bE_{\bx}} 
	= \sum_{k=1}^l \underset{m \times 1}{\btau_k} \cdot \underset{1 \times n}{\bp_k^{\T}} + \underset{m \times n}{\bE_{\bx}},\\
	\label{ch1:eq:PLS_Y}
	\underset{m \times r}{\bY} 
	&= \underset{m \times l}{\bU} \cdot \underset{l \times r}{\bQ} + \underset{m \times r}{\bE_{\by}}
	=  \sum_{k=1}^l  \underset{m \times 1}{\bnu_k} \cdot \underset{1 \times r}{\bq_k^{\T}} +  \underset{m \times r}{\bE_{\by}}.
\end{align}
Здесь $\bT$ и $\bU$~--- образы исходных матриц в скрытом пространстве, причём столбцы матрицы $\bT$ ортогональны; $\bP$ и $\bQ$~--- матрицы перехода; $\bE_{\bx}$ и $\bE_{\by}$~--- матрицы остатков. 
Метод PLS восстанавливает линейную зависимость между столбцами матриц~$\bT$ и~$\bU$
\begin{equation*}
	\bU \approx \bT \bB, \quad \bB = \text{diag}(\beta_k), \quad \beta_k = \bnu_k^{\T}\btau_k / (\btau_k^{\T}\btau_k),
\end{equation*}
где $\{\btau_k\}_{k=1}^l$, $\{\bnu_k\}_{k=1}^l$~--- столбцы матриц $\bT$ и $\bU$ соответственно.

Метод решает следующую оптимизационную задачу:
\begin{equation}
	\max_{\|\bp\|_{2}=\|\bq\|_{2}=1}[ \text{cov}(\bX \bp, \bY \bq)^{2}] = \max_{\bp, \bq} \frac{\bp^{\T} \bX^{\T} \bY \bq}{\sqrt{\bp^{\T} \bp} \sqrt{\bq^{\T} \bq}}.
	\label{ch1:eq:pls_max_cov}
\end{equation}

\vspace{0.5cm}
\textbf{Канонический анализ корреляций для задачи декодирования.}
Оптимизационная задача канонического корреляционного анализа (CCA) похожа на оптимизационную задачу PLS~\eqref{ch1:eq:pls_max_cov} с той лишь разницей, что вместо максимизации ковариации CCA максимизирует корреляцию:
\begin{equation}
	\max_{\|\bp\|_{2}=\|\bq\|_{2}=1}[ \text{corr}(\bX \bp, \bY \bq)^{2}] = \max_{\bp, \bq} \frac{\bp^{\T} \bX^{\T} \bY \bq}{\sqrt{\bp^{\T} \bX^{\T}  \bX \bp} \sqrt{\bq^{\T} \bY^{\T}  \bY\bq}}.
	\label{ch1:eq:cca_max_corr}
\end{equation}

Метод DeepCCA максимизирует корреляцию между представлениями, полученными на выходе нейросети:
\begin{multline}
	\max_{\|\bp\|_{2}=\|\bq\|_{2}=1}\left[ \text{corr}(\mathbf{G}_{\bx}(\bX, \bW_{\bx}) \cdot \bp, \mathbf{G}_{\by}(\bY, \bW_{\by}) \cdot \bq)^{2}\right] = \\ = \max_{\bp, \bq} \frac{\bp^{\T} \mathbf{G}_{\bx}(\bX, \bW_{\bx})^{\T} \mathbf{G}_{\by}(\bY, \bW_{\by}) \bq}{\sqrt{\bp^{\T} \mathbf{G}_{\bx}(\bX, \bW_{\bx})^{\T}  \mathbf{G}_{\bx}(\bX, \bW_{\bx}) \bp} \sqrt{\bq^{\T} \mathbf{G}_{\by}(\bY, \bW_{\by})^{\T}  \mathbf{G}_{\by}(\bY, \bW_{\by}) \bq}}.
	\label{ch1:eq:depp_cca_max_corr}
\end{multline}
Здесь $\mathbf{G}_{\bx}(\bX, \bW_{\bx})$ и $\mathbf{G}_{\by}(\bY, \bW_{\by})$~--- нелинейные проекции исходных и целевых объектов.
С использованием нейросетевых функций модель декодирования способна учитывать существенно нелинейные зависимости как в исходном пространстве, так и в целевом пространстве.

В \textbf{главе 2} приводится формальная постановка задачи построения согласованных моделей декодирования. 
Вводятся понятия скрытого пространства и процедуры согласования образов.
Доказываются теоремы о выборе оптимальной модели декодирования.

\begin{assumption}
	Пусть пространства $\bbX$ и $\bbY$ имеют избыточную размерность. 
	В простейшем случае такими многообразия могут являться вложениями или линейными подпространствами.
\end{assumption}

\begin{definition}
	Назовём пространство $\bbT \subset \bbR^l$ \textit{скрытым пространством} для пространства $\bbX \subset \bbR^n$ ($l \leq n$), если существуют \textit{функция кодирования} $\bphi_{\bx}: \bbX \to \bbT$ и \textit{функция декодирования} $\bpsi_{\bx}: \bbT  \to \bbX$, такие что
	\[
		\text{для любого } \bx \in \bbX \quad \text{существует } \bt \in \bbT: \bpsi_{\bx} (\bphi_{\bx}(\bx)) = \bpsi_{\bx}(\bt) = \bx.
	\]
	
	Аналогично введём определение \textit{скрытого пространства}~$\bbU \subset \bbR^s$ для целевого пространства $\bbY$, \textit{функции кодирования} $\bphi_{\by}: \bbY \to \bbU$ и \textit{декодирования}~$\bpsi_{\by}: \bbU  \to \bbY$, такие что
	\[
	 	\text{для любого } \by \in \bbY \quad \text{существует } \bu \in \bbU: \bpsi_{\by} (\bphi_{\by}(\by)) = \bpsi_{\by}(\bu) = \by.
	\]
\end{definition}

Образы матрицы исходных объектов~$\bX$ и матрицы целевых объектов~$\bY$ в скрытых пространствах~$\bbT$ и $\bbU$ имеют вид
\begin{align*}
	\bT &= \bphi_{\bx} (\bX) = [\bt_1, \dots, \bt_n]^{\T} = [\btau_1, \dots, \btau_l], \\
	\bU &= \bphi_{\by} (\bU) = [\bu_1, \dots, \bu_n]^{\T} = [\bnu_1, \dots, \bnu_s].
\end{align*}

\begin{definition}
	Будем говорить, что скрытые пространства $\bbT$ и $\bbU$ являются \textit{согласованными}, если существует \textit{функция связи} $\bh: \bbT \rightarrow \bbU$, такая что
	\[
		\text{для любого } \bu \in \bbU \quad \text{существует } \bt \in \bbT:  \bu = \bh(\bt).
	\]
\end{definition}

\begin{assumption}
	Предположим, что в задаче прогнозирования~\eqref{ch1:eq:loss_min} пространства $\bbT$ и $\bbU$ являются скрытыми для пространств $\bbX$ и $\bbY$ соответственно. 
	Предположим также, что для данных скрытых пространств $\bbT$ и $\bbU$ существует функция связи~$\bh: \bbT \rightarrow \bbU$. Тогда выполнено
	\[
		\text{для любого } \by \in \bbY \quad \text{существует } \bx \in \bbX: \by = \bpsi_{\by}(\bu) = \bpsi_{\by}(\bh(\bt)) = \bpsi_{\by}(\bh(\bphi_{\bx}(\bx))),
	\]
	и общая схема задачи поиска согласованной модели декодирования принимает вид следующей коммутативной диаграммы:
	\begin{equation}
		\begin{tikzpicture}
			\matrix (m) [matrix of math nodes,row sep=3em,column sep=4em,minimum width=2em]
			{
				\bbX \subset \bbR^n & \bbY \subset \bbR^r \\
				\bbT \subset \bbR^\ell & \bbU \subset \bbR^s \\};
			\path[-stealth]
			(m-1-1) edge [black] node [black, above] {$\mathbf{f}$} (m-1-2)
			(m-2-1) edge [black, bend right=10] node [black, right] {$\bpsi_{\bx}$} (m-1-1)
			(m-2-2) edge [black, bend left=10] node [black, left] {$\bpsi_{\by}$} (m-1-2)
			(m-1-1) edge [black, bend right=10] node [black, left] {$\bphi_{\bx}$} (m-2-1)
			(m-1-2) edge [black, bend left=10] node [black, right] {$\bphi_{\by}$} (m-2-2)
			(m-2-1) edge [black] node [black, above] {$\mathbf{h}$} (m-2-2);
		\end{tikzpicture}
		\label{ch2:eq:decoding_scheme}
	\end{equation}
\end{assumption}

\begin{definition}
	Согласно диаграмме~\eqref{ch2:eq:decoding_scheme}, определим \textit{согласованную} модель декодирования $\mathbf{f}: \bbX \rightarrow \bbY$ как суперпозицию
	\begin{equation}
		\mathbf{f} = \bpsi_{\by} \circ \bh \circ \bphi_{\bx}.
		\label{ch2:eq:def_decoding_function}
	\end{equation}
\end{definition}

Для поиска оптимальных параметров функций кодирования $\bphi_{\bx}$ и $\bphi_{\by}$, декодирования $\bpsi_{\bx}$ и $\bpsi_{\by}$, а также функции связи~$\bh$ ставится задача максимизации $\textit{функции согласования скрытых векторов}$
\[
	g: \bbR^m \times \bbR^m \rightarrow \bbR, \quad g(\btau, \bnu) \rightarrow \max_{\bphi_{\bx}, \bphi_{\by}, \bh}.
\]
Каждая пара скрытых векторов $\btau, \bnu$ ищется последовательно.

\textbf{Метод главных компонент.} 
Функции кодирования $\bphi_{\bx}: \bbR^n \to \bbR^l$ и декодирования $\bpsi_{\bx}: \bbR^l \to \bbR^n$ имеют вид
\begin{equation*}
	\bphi_{\bx}(\bX) =  \underset{m \times n}\bX \cdot \underset{n \times l}\bP^{\T}, \quad	\bpsi_{\bx}(\bT) =  \underset{m \times l}\bT \cdot \underset{l \times n}\bP,
	\label{ch2:eq:PCA2}
\end{equation*}
где $\bP = [\bp_1, \dots, \bp_l]^{\T}$, при этом $\bP \bP^{\T} = \bI$.

Скрытые вектора $\btau$ строятся так, чтобы выборочная дисперсия столбцов проекций матрицы~$\bX$ была максимальной:
\begin{equation*}
	\bp = \argmax_{\|\bp\|_{2} = 1} g(\btau) = \argmax_{\|\bp\|_{2} = 1} [\text{var}(\btau)] = \argmax_{\|\bp\|_{2} = 1} [\text{var}(\bX \textbf{p})].
\end{equation*}

Метод PCA не согласует исходные переменные и целевые переменные. 
А именно метод PCA не находит функции кодирования $\bphi_{\by}$ и декодирования $\bpsi_{\by}$, а также функцию связи $\bh$. 
При этом функция согласования скрытых векторов $g(\btau)$ зависит только от одного аргумента.

\textbf{Метод частичных наименьших квадратов и канонический анализ корреляций.}
В методах PLS и CCA функции кодирования и декодирования имеют вид
\begin{align*}
	\bphi_{\bx}(\bX) &= \bX \bW, \quad \bphi_{\by}(\bY) = \bY \bC, \\
	\bpsi_{\bx}(\bT) &= \bT \bP^{\T}, \quad \bpsi_{\by}(\bU) = \bU \bQ^{\T}.
\end{align*}
Функция связи $\bh$ имеет вид линейной модели, связывающей образы проекций в скрытом пространстве $\bu = \bh(\bt) = \bB^{\T} \bt$.
В данном случае схема декодирования~\eqref{ch2:eq:decoding_scheme} принимает вид следующей коммутативной диаграммы.
\begin{equation*}
	\begin{tikzpicture}
		\matrix (m) [matrix of math nodes,row sep=3em,column sep=4em,minimum width=2em]
		{
			\bbX \subset \bbR^n & \bbY \subset \bbR^r \\
			\bbT \subset \bbR^\ell & \bbU \subset \bbR^s \\};
		\path[-stealth]
		(m-1-1) edge [black] node [black, above] {$\mathbf{f}$} (m-1-2)
		(m-1-1) edge [black, bend right=10] node [black, left] {$\bW$} (m-2-1)
		(m-2-1) edge [black, bend right=10] node [black, right] {$\bP$} (m-1-1)
		(m-1-2) edge [black, bend left=10] node [black, right] {$\bC$} (m-2-2)
		(m-2-2) edge [black, bend left=10] node [black, left] {$\bQ$} (m-1-2)
		(m-2-1) edge [black] node [black, above] {$\bB$} (m-2-2);
	\end{tikzpicture}
\end{equation*}
Для метода PLS функция согласования скрытых векторов имеет вид $g(\btau, \bnu) = \text{cov}(\btau, \bnu)$, а для метода CCA: $g(\btau, \bnu) = \text{corr}(\btau, \bnu)$.

\textbf{Нелинейный канонический анализ корреляций.}
Функции кодирования и декодирования являются нелинейными нейросетями вида
\begin{align*}
	\bT &= \bphi_{\bx}(\bX) =  \bW_\bx^L \sigma(\dots \sigma(\bW_\bx^2 \sigma(\bX \bW_\bx^1)) \dots ), \\
	\bU &= \bphi_{\by}(\bY) =  \bW_\by^L \sigma(\dots \sigma(\bW_\by^2 \sigma(\bY \bW_\by^1)) \dots ), \\
	\bX &= \bpsi_{\bx}(\bT) =  \bW_\bt^L \sigma(\dots \sigma(\bW_\bt^2 \sigma(\bT \bW_\bt^1)) \dots ), \\
	\bY &= \bpsi_{\by}(\bU) =  \bW_\bu^L \sigma(\dots \sigma(\bW_\bu^2 \sigma(\bU \bW_\bu^1)) \dots ).
\end{align*}
Каждая нейросеть является суперпозицией последовательных умножений на матрицы параметров и применения поэлементных функций активаций.

Требуется найти такие параметры, при которых функция согласования~$g$ достигает своего максимума:
\begin{equation}
	g(\btau, \bnu) \rightarrow \max_{\bW},
	\label{ch2:eq:concordance}
\end{equation}
где $\bW = \bigl\{\bW_\bx^i, \bW_\by^i, \bW_\bt^i, \bW_\bu^i\bigr\}_{i=1}^L$.

При использовании в качестве функции согласования корреляции между проекциями $g(\btau, \bnu) = \textnormal{corr}(\btau, \bnu)$ частная производная функции согласования по первому аргументу принимает вид
\[
	\frac{\partial \mathbf{g}(\bT, \bU)}{\partial \bT} = \frac{1}{m - 1} \left(\bSigma_1^{-1/2} \bU \bV^{\T} \bSigma_2^{-1/2} \bU - \bSigma_1^{-1/2} \bU \bD \bV^{\T} \bSigma_1^{-1/2} \right),
\]
где $\bU, \bD, \bV = \textnormal{SVD}(\bSigma)$, \, $\bSigma = \bSigma_1^{-1/2} \bSigma_{12} \bSigma_2^{-1/2} $, \, $\bSigma_1 = \frac{1}{m - 1} \bT \bT^{\T}$, \, $\bSigma_2 = \frac{1}{m - 1} \bU \bU^{\T}$, \, $\bSigma_{12} = \frac{1}{m - 1} \bT \bU^{\T}$.
Аналогичное выражение имеет частная производная по второму аргументу.
Полученное выражение для градиента позволяет построить эффективный алгоритм для решения задачи с использованием градиентных методов оптимизации.

Алгоритм PLS итеративно на каждом из $l$ шагов вычисляет по одному столбцу $\btau_k$, $\bnu_k$ матриц $\bT$, $\bU$ и по одной строке $\bp_k$, $\bq_k$ матриц $\bP$, $\bQ$ соответственно. 

\begin{statement}
	Максимизация ковариации между векторами $\btau_k$ и $\bnu_k$ сохраняет дисперсию столбцов матриц~$\bX$ и~$\bY$ и учитывает их линейную зависимость.
\end{statement}

\begin{statement}
	Вычисленные вектора $\bw_k$ и $\bc_k$ с помощью итеративной процедуры обновления:
	\begin{align}
		\btau_k &:= \frac{\bX_k \bw_k}{\|\bw_k\|}, \quad  \bw_k := \frac{\bX_k^{\T} \bnu_{k-1}}{\bnu_{k-1}^{\T} \bnu_{k-1}}; \label{ch2:eq:pls_step1}\\ 
		\bnu_k &:= \frac{\bY_k \bc_k}{\| \bc_k \|}, \quad \bc_k := \frac{\bY_k^{\T} \btau_k}{\btau_k^{\T} \btau_k}. \label{ch2:eq:pls_step2}
	\end{align}
	будут собственными векторами матриц $\bX_k^{\T} \bY_k \bY_k^{\T} \bX_k$ и $\bY_k^{\T} \bX_k \bX_k^{\T} \bY_k$, соответствующими максимальным собственным значениям.
	\begin{equation*}
		\bw_k \varpropto \bX_k^{\T} \bnu_{k-1} \varpropto \bX_k^{\T} \bY_k \bc_{k-1} \varpropto \bX_k^{\T} \bY_k \bY_k^{\T} \btau_{k-1} \varpropto \bX_k^{\T} \bY_k \bY_k^{\T} \bX_k \bw_{k-1},
	\end{equation*}
	\begin{equation*}
		\bc_k \varpropto \bY_k^{\T} \btau_k \varpropto \bY_k^{\T} \bX_k \bw_k \varpropto \bY_k^{\T} \bX_k \bX_k^{\T} \bnu_{k-1} \varpropto \bY_k^{\T} \bX_k \bX_k^{\T} \bY_k \bc_{k-1},
	\end{equation*}
	где символ $\varpropto$ означает равенство с точностью до мультипликативной константы. 
	\label{ch2:stat:eig}
\end{statement}

\begin{statement}
		Вычисленные вектора $\btau_k$ и $\bnu_k$ с помощью итеративной процедуры обновления~\eqref{ch2:eq:pls_step1},~\eqref{ch2:eq:pls_step2} обладают максимальной ковариацией $\textnormal{cov}(\btau, \bnu)$.
\end{statement}

Для перехода на следующий шаг необходимо вычесть из матриц $\bX_k$ и $\bY_k$ одноранговые аппроксимации $\btau_k \bp_k^{\T}$ и $\btau_k \bq_k^{\T}$
\begin{align*}
	\bX_{k + 1} &= \bX_{k} - \btau_k \bp_k^{\T} = \bX - \sum_k \btau_k \bp_k^{\T}, \\
	\bY_{k + 1} &= \bY_{k} - \btau_k \bq_k^{\T} = \bY - \sum_k \btau_k \bq_k^{\T}.
\end{align*}

\begin{theorem}
	В случае линейных функций декодирования $\bpsi_{\bx}(\bT) = \bT \bP$, $\bpsi_{\by} (\bU) = \bU \bQ$ и функции согласования $g(\btau, \bnu) = \textnormal{cov}(\btau, \bnu)$ параметры
	\[
		\bTheta = \bW (\bP^{\T} \bW)^{-1} \bB \bQ
	\]
	являются оптимальными для модели~\eqref{ch1:eq:lin_reg_model}.
\end{theorem}

Финальная модель является линейной, низкоразмерной в скрытом пространстве. 
Это снижает избыточность данных и повышает стабильность модели.

Пусть $\mathbf{f}_1(\bx_1, \bTheta_1)$, $\mathbf{f}_2(\bx_2, \bTheta_2)$~--- линейные модели декодирования сигналов. 
\begin{definition}
	Назовём \textit{аддитивной суперпозицией} моделей декодирования модель~\eqref{ch1:eq:lin_reg_model} вида
	\begin{equation}
		\bY = \mathbf{f}(\bx, \bTheta) + \bepsilon = \mathbf{f}_1(\bx_1, \bTheta_1) + \mathbf{f}_2(\bx_2, \bTheta_2) + \bepsilon = \bTheta_1^{\T} \bx_1 + \bTheta_2^{\T} \bx_2 + \bepsilon,
		\label{ch2:eq:two_modal_reg_model}
	\end{equation}
	где объект $\bx = [\bx_1, \bx_2] \in \bbR^n$ состоит из двух подвекторов $\bx_1 \in \bbR^k$, $\bx_2 \in \bbR^{n - k}$. 
\end{definition}

\begin{statement}
	\label{ch2:stat:two_modal_params}
	Оптимальная матрица параметров $\bTheta$ для модели~\eqref{ch2:eq:two_modal_reg_model}, доставляющая минимум функции ошибки~\eqref{ch1:eq:l2_loss_function}, имеет вид:
	\begin{align*}
		\bTheta_1 &= (\bX_1^{\T} \bM_{\bX_2} \bX_1)^{-1} \bX_1^{\T} \bM_{\bX_2} \bY, \\
		\bTheta_2 &= (\bX_2^{\T} \bM_{\bX_1} \bX_2)^{-1} \bX_2^{\T} \bM_{\bX_1} \bY,
	\end{align*}
	где $\bM_{\bX_1} = (\bI - \bP_{\bX_1})$, $\bM_{\bX_2} = (\bI - \bP_{\bX_2})$, $\bP_{\bX_1} = \bX_1 (\bX_1^{\T} \bX_1)^{-1} \bX_1^{\T}$, $\bP_{\bX_2} = \bX_2 (\bX_2^{\T} \bX_2)^{-1} \bX_2^{\T}$.
\end{statement}

\begin{statement}
	\label{ch2:stat:two_modal_theta2}
	Оптимальная подматрица $\bTheta_2$ в модели~\eqref{ch2:eq:two_modal_reg_model} является решением задачи регрессии
	\begin{equation}
		\| \bY_1 -  \bX_{21} \bTheta_2 \|^2 \rightarrow \min_{\bTheta_2},
		\label{ch2:eq:two_modal_fwl}
	\end{equation}
	где $\bY_1 = \bM_{\bX_1} \bY$, $\bX_{21} = \bM_{\bX_1} \bX_2$.
\end{statement}

\begin{statement}
	Если в задаче~\eqref{ch2:eq:two_modal_reg_model} $\text{span}(\bX_1) \cap \text{ span}(\bX_2) = \emptyset$, то есть столбцы матрицы $\bX_1$ ортогональны столбцам матрицы~$\bX_2$, то $\bTheta_2$ является решением задачи регрессии
	\[
		\| \bY -  \bX_{2} \bTheta_2 \|^2 \rightarrow \min_{\bTheta_2}.
	\]
\end{statement}

\begin{statement}
	Ошибка аддитивной суперпозиции моделей не превышает ошибки каждой из отдельных моделей
	\begin{align*}
		\mathcal{L}(\bTheta^*, \bX, \bY) &\leq \mathcal{L}(\bTheta_1, \bX_1, \bY), \\
		\mathcal{L}(\bTheta^*, \bX, \bY) &\leq \mathcal{L}(\bTheta_2, \bX_2, \bY).
	\end{align*}
\end{statement}

\begin{statement}
	\label{ch2:stat:strict:two_model}
	Пусть для аддитивной суперпозиции моделей~\eqref{ch2:eq:two_modal_reg_model} выполнены следующие условия
	\[
		\bY \neq \bP_{\bX_2} \bY, \quad \bX_1 \neq \bP_{\bX_2} \bX_1, \quad \bY^{\T} \bM_{\bX_2} \bX_1 \neq 0.
	\]
	Тогда выполнено строгое неравенство
	\[
		\cL(\bTheta^*, \bX, \bY) < \cL(\bTheta_2, \bX_2, \bY).
	\]
\end{statement}

\textbf{Глава 3} посвящена задаче выбора признаков для задачи декодирования сигналов.
Задача выбора признаков заключается в поиске оптимального подмножества~$\cA \subset \{ 1, \dots, n \}$ индексов признаков среди всех возможных $2^n - 1$ вариантов. 
Существует взаимооднозначное отображение между подмножеством $\cA$ и булевым вектором~$\ba \in \{0, 1\}^n$, компоненты которого указывают, выбран ли признак. 
Для нахождения оптимального вектора~$\ba$ введем функцию ошибки выбора признаков~$S(\ba, \bX, \bY)$:
\begin{equation}
	\ba = \argmin_{\ba' \in \{0, 1\}^n} S(\ba', \bX, \bY).
	\label{ch3:eq:feature_selection}
\end{equation}
Целью выбора признаков является построение функции~$S (\ba, \bX, \bY)$. 

Для решения задачи~\eqref{ch3:eq:feature_selection} применяется релаксация задачи~\eqref{ch3:eq:feature_selection} к непрерывной области определения~$[0, 1]^n$:
\begin{equation}
	\bz = \argmin_{\bz' \in [0, 1]^n} S(\bz', \bX, \bY).
	\label{ch3:eq:relaxed_feature_selection}
\end{equation}
Решение~\eqref{ch3:eq:feature_selection} восстанавливается с помощью отсечения по порогу:
\begin{equation}
	\ba = [a_j]_{j=1}^n, \quad 
	a_j = \begin{cases}
		1, & z_j > \tau; \\
		0, & \text{в противном случае}.
	\end{cases}
	\label{ch3:eq:feature_threshold}
\end{equation}

Как только решение~$\ba$ задачи~\eqref{ch3:eq:feature_selection} получено, задача~\eqref{ch1:eq:l2_loss_function} принимает вид:
\begin{equation*}
	\mathcal{L}(\bTheta_{\cA}, \bX_{\cA}, \bY) = {\left\| \mathbf{Y} - \bX_{\cA}\bTheta_{\cA} \right\| }_2^2 \rightarrow\min_{\bTheta_{\cA}}.
\end{equation*}

Если между столбцами матрицы исходных объектов~$\bX$ существует линейная зависимость, то решение задачи линейной регрессии
\begin{equation}
	\| \bupsilon - \bX \btheta\|_2^2 \rightarrow\min_{\btheta \in \bbR^{n}}.
	\label{ch3:eq:linear_regression}
\end{equation}
оказывается неустойчивым. 
Методы выбора признаков находят подмножество~$ \cA \in \{1, \dots, n\}$ оптимальных столбцов матрицы~$\bX$. 

Метод QPFS выбирает некоррелированные признаки, релевантные целевому вектору~$\bupsilon$.
Введем две функции: $\text{Sim}(\bX)$ контролирует избыточность между признаками, $\text{Rel}(\bX, \bupsilon)$ содержит релевантности между каждым признаком и целевым вектором. 

Метод QPFS минимизирует следующую функцию ошибки
\begin{equation}
	\underbrace{\bz^{\T} \bQ \bz}_{\text{Sim}} - \alpha \cdot \underbrace{\vphantom{()} \mathbf{b}^{\T} \bz}_{\text{Rel}} \rightarrow \min_{\substack{\bz \in \bbR^n_+ \\ \|\bz\|_1=1}}.
	\label{ch3:eq:qpfs_problem}
\end{equation}
Элементы матрицы парных взаимодействий~$\bQ \in \bbR^{n \times n}$ содержат коэффициенты попарного сходства между признаками. 
Вектор релевантностей признаков~$\mathbf{b} \in \bbR^n$ выражает сходство между каждым признаком и целевым вектором~$\bupsilon$.
Нормированный вектор~$\bz$ отражает значимость каждого признака. 
Параметр~$\alpha$ позволяет контролировать компромисс между Sim и Rel:
\begin{equation*}
	\alpha = \frac{\overline{\bQ}}{\overline{\bQ} + \overline{\bb}}, \quad \text{где}\,\,\overline{\bQ} = \text{mean} (\bQ), \,\,\, \overline{\bb}= \text{mean} (\bb).
\end{equation*}

Для измерения сходства используется выборочный коэффициент корреляции Пирсона между парами признаков для функции Sim, и между признаками и целевым вектором для функции Rel:
\begin{equation}
	\bQ = \left[|\text{corr}(\bchi_i, \bchi_j)|\right]_{i,j=1}^n, \quad \bb = \left[|\text{corr}(\bchi_i, \bupsilon)|\right]_{i=1}^n.
	\label{ch3:eq:qpfs_1d_qb}
\end{equation}
Здесь
\begin{equation*}
\text{corr}(\bchi, \bupsilon) = \frac{\sum_{i=1}^m(\bchi_i - \overline{\bchi})( \bupsilon_i - \overline{\bupsilon})}{\sqrt{\sum_{i=1}^m(\bchi_i - \overline{\bchi})^2\sum_{i=1}^m(\bupsilon_i - \overline{\bupsilon})^2}}.
\end{equation*}

В случае векторной целевой переменной компоненты целевой переменной могут коррелировать между собой. 
Предлагаются методы, учитывающие зависимости как в исходном, так и в целевом пространствах.

\textbf{Агрегация релевантностей целевых векторов.}
Чтобы применить метод QPFS к векторному случаю ($r > 1$), релевантности признаков агрегируются по всем $r$ компонентам целевой переменной. 
Вектор релевантностей $\bb$ агрегируется по всем компонентам целевой переменной и определяется как
\begin{equation*}
	\bb = \left[\sum_{k=1}^r|\text{corr}(\bchi_i, \bupsilon_k)|\right]_{i=1}^n.
\end{equation*}
Недостатком такого подхода является отсутствие учёта зависимостей в столбцах матрицы~$\bY$. 

\textbf{Симметричный учёт значимости признаков и целевых переменных.}
Добавим член~$\text{Sim}(\bY)$ и изменим член $\text{Rel}(\bX, \bY)$ следующим образом:
\begin{equation}
	\alpha_1 \cdot \underbrace{\bz_x^{\T} \bQ_x \bz_x}_{\text{Sim}(\bX)} - \alpha_2 \cdot \underbrace{\bz_x^{\T} \bB \bz_y}_{\text{Rel}(\bX, \bY)} + \alpha_3 \cdot \underbrace{\bz_y^{\T} \bQ_y \bz_y}_{\text{Sim}(\bY)} \rightarrow \min_{\substack{\bz_x \geq \bZero_n, \, \bOne_n^{\T}\bz_x=1 \\ \bz_y \geq \bZero_r, \, \bOne_r^{\T}\bz_y=1}}.
	\label{ch3:eq:symimp}
\end{equation}
Определим элементы матриц~$\bQ_x \in \bbR^{n \times n}$, $\bQ_y \in \bbR^{r \times r}$ и $\bB \in \bbR^{n \times r}$ следующим образом:
\begin{equation*}
	\bQ_x = \left[ |\text{corr}(\bchi_i, \bchi_j)| \right]_{i,j=1}^n, \quad
	\bQ_y = \left[ |\text{corr}(\bupsilon_i, \bupsilon_j)| \right]_{i,j=1}^r, \quad
	\bB =  \left[ |\text{corr}(\bchi_i, \bupsilon_j)| \right]_{\substack{i=1, \dots, n \\ j=1, \dots, r}}.
\end{equation*}
Вектор~$\bz_x$ содержит коэффициенты значимости признаков, $\bz_y$~--- коэффициенты значимости целевых векторов.

\begin{statement}
	Баланс между~$\text{Sim}(\bX)$, $\text{Rel}(\bX, \bY)$ и $\text{Sim}(\bY)$ в  задаче~\eqref{ch3:eq:symimp} достигается при:
	\begin{equation}
	\alpha_1 \propto \overline{\bQ}_y \overline{\bB}, \quad
	\alpha_2 \propto \overline{\bQ}_x \overline{\bQ}_y, \quad
	\alpha_3  \propto \overline{\bQ}_x \overline{\bB}.
	\label{ch3:eq:alpha_3}
	\end{equation}
\end{statement}
Здесь~$\overline{\bQ}_x$, $\overline{\bB}$ и $\overline{\bQ}_y$~--- средние значения соответствующих матриц~$\bQ_x$, $\bB$ и $\bQ_y$ членов~$\text{Sim}(\bX)$, $\text{Rel}(\bX, \bY)$ и $\text{Sim}(\bY)$.

\textbf{Минимаксная постановка задачи выбора признаков.}
Сформулируем две взаимосвязанные задачи:
\begin{align}
	\alpha_1 \cdot \underbrace{\bz_x^{\T} \bQ_x \bz_x}_{\text{Sim}(\bX)} - \alpha_2 \cdot \underbrace{ \bz_x^{\T}\mathbf{B} \bz_y}_{\text{Rel}(\bX, \bY)} \rightarrow \min_{\substack{\bz_x \geq \bZero_n, \\ \bOne_n^{\T}\bz_x=1}},
	\label{ch3:eq:x_qpfs}\\
	\alpha_3 \cdot \underbrace{\bz_y^{\T} \bQ_y \bz_y}_{\text{Sim}(\bY)} + \alpha_2 \cdot \underbrace{ \bz_x^{\T} \mathbf{B} \bz_y}_{\text{Rel}(\bX, \bY)} \rightarrow \min_{\substack{\bz_y \geq \bZero_r,  \\ \bOne_r^{\T}\bz_y=1}}.
	\label{ch3:eq:y_qpfs}
\end{align}
Задачи~\eqref{ch3:eq:x_qpfs} и \eqref{ch3:eq:y_qpfs} объединяются в совместную минимакс или максмин постановку
\begin{equation}
	\min_{\substack{\bz_x \geq \bZero_n \\ \bOne_n^{\T}\bz_x=1}} 	\max_{\substack{\bz_y \geq \bZero_r \\ \bOne_r^{\T}\bz_y=1}} f(\bz_x, \bz_y), \quad \left(\text {или} \, \max_{\substack{\bz_y \geq \bZero_r \\ \bOne_r^{\T}\bz_y=1}} \min_{\substack{\bz_x \geq \bZero_n \\ \bOne_n^{\T}\bz_x=1}} f(\bz_x, \bz_y)\right),
	\label{ch3:eq:minmax}
\end{equation}
где
\begin{equation*}
	f(\bz_x, \bz_y) = \alpha_1 \cdot \underbrace{\bz_x^{\T} \bQ_x \bz_x}_{\text{Sim}(\bX)} - \alpha_2 \cdot \underbrace{\bz_x^{\T} \bB \bz_y}_{\text{Rel}(\bX, \bY)} - \alpha_3 \cdot \underbrace{\bz_y^{\T} \bQ_y \bz_y}_{\text{Sim}(\bY)}.
\end{equation*}
\begin{theorem}
	Для положительно определенной матрицы~$\bQ_x$ и $\bQ_y$, максмин и минимакс задачи~\eqref{ch3:eq:minmax} имеют одинаковое оптимальное значение.
\end{theorem}

\begin{statement}
	Минимаксная задача~\eqref{ch3:eq:minmax} эквивалентна задаче квадратичного программирования с~$n + r + 1$ переменными.
\end{statement}

\textbf{Несимметричный учёт значимостей признаков и целевых переменных.}
Добавим линейный член~$\bb^{\T} \bz_y$ в член $\text{Rel}(\bX, \bY)$ следующим образом:
\begin{equation}
	\alpha_1 \cdot \underbrace{\bz_x^{\T} \bQ_x \bz_x}_{\text{Sim}(\bX)} - \alpha_2 \cdot  \underbrace{\left(\bz_x^{\T} \bB \bz_y - \bb^{\T} \bz_y \right) }_{\text{Rel}(\bX, \bY)} + \alpha_3 \cdot \underbrace{\bz_y^{\T} \bQ_y \bz_y}_{\text{Sim}(\bY)} \rightarrow \min_{\substack{\bz_x \geq \bZero_n, \, \bOne_n^{\T}\bz_x=1 \\ \bz_y \geq \bZero_r, \, \bOne_r^{\T}\bz_y=1}}.
	\label{ch3:eq:asymimp}
\end{equation}

\begin{statement}
	Пусть вектор $\bb$ равен
	\begin{equation*}
	b_j = \max_{i=1, \dots n} [\bB]_{i, j}.
	\end{equation*}
	Тогда значение коэффициентов значимостей вектора~$\bz_y$ будут неотрицательными в~$\text{Rel}(\bX, \bY)$ для задачи~\eqref{ch3:eq:asymimp}.
\end{statement}

\begin{statement}
	Баланс между членами~$\text{Sim}(\bX)$, $\text{Rel}(\bX, \bY)$ и $\text{Rel}(\bX, \bY)$ для задачи~\eqref{ch3:eq:asymimp} достигается при следующих коэффициентах:
	\begin{equation*}
		\alpha_1 \propto \overline{\bQ}_y \left( \overline{\bb} - \overline{\bB}\right), \quad
		\alpha_2 \propto \overline{\bQ}_x \overline{\bQ}_y, \quad
		\alpha_3  \propto \overline{\bQ}_x \overline{\bB}.
	\end{equation*}
\end{statement}

\begin{theorem}
	В случае скалярной целевой переменной ($r=1$) предлагаемые методы выбора признаков SymImp~\eqref{ch3:eq:symimp}, MinMax~\eqref{ch3:eq:minmax}, AsymImp~\eqref{ch3:eq:asymimp} совпадают с оригинальным методом QPFS~\eqref{ch3:eq:qpfs_problem}.
\end{theorem}

Таблица~\ref{ch3:tbl:summary} демонстрирует основные идеи и функции ошибок для каждого метода. 
RelAgg является базовой стратегией и не учитывает корреляции в целевом пространстве.
SymImp штрафует попарные корреляции между целевыми векторами.
MinMax более чувствителен к целевым векторам, которые трудно предсказать.
Стратегия Asymimp добавляет линейный член к функции SymImp, чтобы сделать вклад признаков и целевых векторов асимметричным.

\begin{table}[ht]
	\centering
	\caption{Обзор предлагаемых обобщений метода QPFS для векторной целевой переменной}
	\small{
		\begin{tabular}{c|c|c}
			\hline
			Метод & Идея & Функция ошибки $S(\bz | \bX, \bY)$ \\
			\hline && \\ [-.5em]
			RelAgg & $\min \bigl[ \text{Sim}(\bX) - \text{Rel}(\bX, \bY) \bigr] $ & $\min\limits_{\bz_x} \bigl[ (1 - \alpha) \cdot \bz_x^{\T} \bQ_x \bz_x - \alpha \cdot \bz_x^{\T} \bB \bOne_r \bigr] $ \\ &&\\[-.5em]
			SymImp & $\begin{aligned} \min \, \bigl[ \text{Sim}(\bX) & - \text{Rel}(\bX, \bY) \\ & + \text{Sim}(\bY) \bigr] \end{aligned}$ & $ \min\limits_{\bz_x, \, \bz_y} \left[ \alpha_1 \cdot \bz_x^{\T} \bQ_x \bz_x - \alpha_2 \cdot \bz_x^{\T} \bB \bz_y + \alpha_3 \cdot \bz_y^{\T} \bQ_y \bz_y \right] $\\ &&\\ [-.5em]
			MinMax & $\begin{aligned} &\min \, \bigl[ \text{Sim}(\bX) - \text{Rel}(\bX, \bY) \bigr]  \\ & \max \bigl[\text{Rel}(\bX, \bY) + \text{Sim}(\bY) \bigr] \end{aligned}$ & $	\min\limits_{\bz_x} 	\max\limits_{\bz_y} \bigl[\alpha_1 \cdot \bz_x^{\T} \bQ_x \bz_x - \alpha_2 \cdot \bz_x^{\T} \bB \bz_y - \alpha_3 \cdot \bz_y^{\T} \bQ_y \bz_y \bigr]$ \\ &&\\ [-.5em]
			AsymImp & $\begin{aligned} & \min \, \bigl[ \text{Sim}(\bX) - \text{Rel}(\bX, \bY) \bigr]\\ &  \max \bigl[\text{Rel}(\bX, \bY) + \text{Sim}(\bY) \bigr] \end{aligned}$ & $\min\limits_{\bz_x, \bz_y} \bigl[ \alpha_1 \cdot \bz_x^{\T} \bQ_x \bz_x - \alpha_2 \cdot \left(\bz_x^{\T} \bB \bz_y - \bb^{\T} \bz_y \right) + \alpha_3 \cdot \bz_y^{\T} \bQ_y \bz_y \bigr]$\\ 
			\hline
	\end{tabular}}
	\label{ch3:tbl:summary}
\end{table}

В \textbf{главе 4} предлагается метод выбора активных параметров модели с использованием метода выбора признаков с помощью квадратичного программирования. 
Приводится анализ параметров модели, которые не находятся в оптимуме.

Модель $f( \bx, \btheta)$ с параметрами $\btheta \in \mathbb{R}^p$ предсказывает целевой объект~$y \in \bbY$ по исходному объекту~$\bx \in \bbR^{n}$. Пространство~$\bbY$ представляет собой бинарные метки классов~$\{0, 1\}$ для задачи двухклассовой классификации и~$\bbR$ для задачи регрессии.
Параметры~$\btheta$ вычисляются минимизацией функции ошибки:
\begin{equation}
	\btheta^* = \argmin_{\btheta \in \bbR^p} \cL(\btheta, \bX, \by).
	\label{ch4:eq:error_function}
\end{equation}
В качестве функции ошибки~$\cL (\btheta, \bX, \by)$ рассматриваются квадратичная ошибка для задачи регрессии:
\begin{equation}
	\cL(\btheta, \bX, \by) = \frac 12 \| \by - \mathbf{f}(\bX, \btheta) \|_2^2 = \frac 12 \sum_{i=1}^m \bigl( y_i - f(\bx_i,  \btheta)\bigr)^2,
	\label{ch4:eq:squared_error}
\end{equation}
и функция кросс-энтропии для задачи бинарной классификации: 
\begin{equation}
	\cL(\btheta, \bX, \by) = \sum_{i=1}^m \bigl[y_i \log f (\bx_i , \btheta) + (1-y_i) \log \bigl(1 - f (\bx_i , \btheta)\bigr)\bigr].
	\label{ch4:eq:log_loss}
\end{equation}

Для выбора вектора обновлений~$\Delta \btheta$ используется метод оптимизации Ньютона .

Метод Ньютона нестабилен и вычислительно сложен. 
В работе предлагается стабильный метод Ньютона. 
Перед шагом градиента предлагается выбрать подмножество активных параметров модели, которые оказывают наибольшее влияние на функцию ошибки~$\cL (\btheta, \bX, \by)$.
\begin{definition}
	\label{ch4:def:active_param}
	Параметр $\theta_j$ для модели $f(\bx, \btheta)$ является \textit{активным}, если $\bJ^{\T} (\mathbf{f}(\bx, \btheta) - \by) \neq 0$.
\end{definition}
Обновление параметров производится только для отобранного множества индексов~$\cA = \bigl\{j: a_j = 1, \ba \in \{0, 1\}^p\bigr\}$
\begin{align*}
	\btheta_{\cA}^k &= \btheta_{\cA}^{k - 1} + \Delta \btheta_{\cA}^{k - 1}, \quad \btheta_{\cA} = \{\theta_j: j \in \cA \}, \\
	\btheta_{\bar{\cA}}^k &= \btheta_{\bar{\cA}}^{k - 1}, \quad \btheta_{\bar{\cA}} = \{\theta_j: j \notin \cA \}.
\end{align*}
Чтобы выбрать оптимальное подмножество индексов~$\cA$, из всех возможных $2^p - 1$~подмножеств, используется метод QPFS. Метод выбирает подмножество параметров~$\ba$ для вектора обновлений~$ \Delta \btheta$, которые оказывают наибольшее влияние на вектор остатков и являются попарно независимыми:
\begin{equation}
	\ba = \argmax_{\ba' \in \{1, 0\}^p} S(\ba', \bX, \by, \btheta) \Leftrightarrow \argmin_{\ba  \in \bbR^p_+, \, \|\ba\|_1=1} \bigl[\ba^{\T} \bQ \ba - \alpha \cdot \mathbf{b}^{\T} \ba \bigr].
\end{equation}

Метод Ньютона использует условие оптимизации первого порядка для задачи~\eqref{ch4:eq:error_function} и линеаризует градиент $S (\btheta)$
\[
	\nabla S (\btheta + \Delta \btheta) = \nabla S(\btheta) + \bH \cdot \Delta \btheta = 0, \quad
	\Delta \btheta = - \bH^{-1} \nabla S(\btheta).
\]
где $\bH = \nabla^2 S(\btheta)$ является матрицой Гессиана функции ошибки $S(\btheta)$.

\vspace{0.2cm}
\begin{theorem}
	Пусть модель $f (\bx , \btheta)$ близка к линейной в окрестности точки $\btheta + \Delta \btheta$
	\begin{equation}
		\mathbf{f}(\bX , \btheta + \Delta \btheta) \approx \mathbf{f}(\bX , \btheta) + \bJ \cdot \Delta  \btheta,
		\label{ch4:eq:linearization}
	\end{equation}
	где $\mathbf{J} \in \bbR^{m \times p}$ является матрицой Якоби.
	Тогда вектор обновления~$\Delta \btheta$ для функции ошибки~\eqref{ch4:eq:squared_error} является решением задачи линейной регрессии
	\begin{equation}
		\| \be - \bF \Delta \btheta \|_2^2 \rightarrow \min_{\Delta \btheta \in \bbR^{p}},
		\label{ch4:eq:lin_reg_nonlin_reg}
	\end{equation}
	где $\be = \mathbf{f} - \by$ и $\bF = \bJ$.
\end{theorem}

\vspace{0.2cm}
\begin{theorem}
Рассмотрим модель логистической регрессии вида $f(\bx , \btheta) = \sigma(\bx^{\T} \btheta)$ с сигмоидной функцией активации~$\sigma(\cdot)$. 
Вектор обновлений $\Delta \btheta$ для функции ошибки~\eqref{ch4:eq:log_loss} является решением задачи линейной регрессии
\begin{equation}
	\| \be - \bF \Delta \btheta \|_2^2 \rightarrow \min_{\Delta \btheta \in \bbR^{p}},
	\label{ch4:eq:lin_reg_log_reg}
\end{equation}
где $\be = \bR^{-1/2} (\by - \mathbf{f})$ и $\bF = \bR^{1/2}\bX$.
\end{theorem}

Предлагается адаптация метода QPFS для решения задач~\eqref{ch4:eq:lin_reg_nonlin_reg} и \eqref{ch4:eq:lin_reg_log_reg}. 
Матрица парных взаимодействий~$\bQ$ и вектор релевантностей~$\bb$ имеют вид
\[
	\bQ = \text{Sim} (\bF), \quad \bb = \text{Rel} (\bF, \be).
\]
\begin{statement}
	В оптимальной точке~$\btheta^*$ вектор релевантностей~$\bb = \text{Rel} (\bF, \be)$ равен нулю.
\end{statement}
\vspace{0.2cm}

В \textbf{главе 5} ставится задача метрического обучения как поиск оптимальной метрики в целевом пространстве.
При использовании в качестве функции ошибки квадратичной ошибки предполагается, что целевое пространство является евклидовым. 
Данное предположение не всегда является адекватным.
Рассматриваются задачи кластеризации и классификации множества временных рядов.

Пусть~$\bX = [\bx_1, \dots, \bx_m]^{\T} \in \mathbb{R}^{m \times n}$~--- матрица исходных объектов.
Требуется выявить кластерную структуру данных и разбить множество объектов~$\bX$ на множество непересекающихся кластеров $\bbY = \{1, \dots, K\}$, т.\,е.\ построить отображение $f: \bbR^n \to \bbY$.
Обозначим~$y_i = f (\bx_i)$, $y_i \in \bbY$~--- метка кластера объекта~$\bx_i$.
\begin{definition}
	\label{ch5:def:centroid}
	\textit{Центроидом} класса $e \in \bbY$ множества объектов $\bX_e$ по расстоянию $\rho$ назовем вектор $\mathbf{c}_e \in \mathbb{R}^n$ такой, что
	\begin{equation}
		\label{ch5:eq:centroid_task}
		\mathbf{c}_e = \mathop{\text{argmin}}\limits_{{\mathbf{c} \in \mathbb{R}^n}}\sum_{\mathbf{x}_i \in \mathbf{X}_e}
		{\rho(\mathbf{x}_i ,\mathbf{c})}.
	\end{equation}
	Здесь $\bX_e = \{\bx_i: i=1, \dots, m, \, y_i=e\}$~--- множество объектов выборки, принадлежащих одному классу $e \in \bbY$.
\end{definition}
В случае использования в качестве расстояния $\rho$ евклидовой метрики формула~\eqref{ch5:eq:centroid_task} принимает вид
\begin{equation}
	\label{ch5:eq:mu}
	\bc =\frac{1}{m} \sum_{i=1}^m\bx_i, \quad
	\bc_e =\frac{ \sum_{i=1}^m [y_i = y_e]\bx_i } {\sum_{i=1}^m [y_i = y_e]}\,.
\end{equation}
Здесь вектор $\bc$ является центроидом всего множества объектов $\bX$.

Введем на множестве объектов~$\bX$ расстояние Махаланобиса
\begin{equation}
	d_{\bA} (\bx_i, \bx_j) = \sqrt{(\bx_i - \bx_j)^{\top} \bA^{-1} (\bx_i - \bx_j)},
	\label{ch5:eq:mahal_distance}
\end{equation}
где \textit{матрица трансформаций} $\mathbf{A} \in \bbR^{n \times n}$ является симметричной и неотрицательно определенной ($\mathbf{A}^{\T} = \mathbf{A}$, $\mathbf{A} \succeq 0$).
Зададим в качестве матрицы трансформации матрицу выборочной ковариации
\begin{equation}
	\label{ch5:eq:covMatrix}
	\bA = \frac{1}{m} \sum_{i=1}^m(\bx_i - \bc)(\bx_i - \bc)^{\T}.
\end{equation}
Функцией ошибки кластеризации назовем межкластерное расстояние:
\begin{equation}
	\cL \bigl(\{\bc_j\}_{j=1}^K, \bX, \by\bigr)= - \sum_{j=1}^K N_j d_{\bA}^2(\bc_j, \bc),
	\label{ch5:eq:cluster_error_function}
\end{equation}
где $N_j = \sum_{i=1}^m [y_i = y_j]$~--- число объектов в кластере~$j$.

Поставим задачу кластеризации как задачу минимизации функции ошибки~\eqref{ch5:eq:cluster_error_function}
\begin{equation}
	\label{ch5:eq:Qmax}
	\cL \bigl(\{\bc_j\}_{j=1}^K, \bX, \by \bigr) \to \min_{\bc_j \in \bbR^{\T}}.
\end{equation}
Найдем такую матрицу~$\bA$, для которой функционал качества принимает максимальное значение:
\begin{equation}
	\label{ch5:eq:Amax}
	\bA^* =\argmin_{\bA \in \bbR^{n \times n}} S \bigl(\{\bc_j^*\}_{j=1}^K, \bX, \by \bigr)\,,
\end{equation}
где $\{\bc_j^*\}_{j=1}^K$~--- решение задачи кластеризации~\eqref{ch5:eq:Qmax}.

Для решения задач~\eqref{ch5:eq:Qmax}, \eqref{ch5:eq:Amax} используется алгоритм адаптивного метрического обучения.
Предлагается понизить размерность пространства объектов~$\bX$ с помощью линейного ортогонального преобразования~$\bP \in \bbR^{l \times n}$, $\bP^{\T} \bP = \mathbf{I}$, где новая размерность $l < n$
\[
	\bt_i = \bP \bx_i \in \bbR^l, \quad i = 1, \dots, m.
\]
Расстояния между объектами вычисляются по формуле~\eqref{ch5:eq:mahal_distance}, где в качестве матрицы~$\hat{\mathbf{A}}$ используется матрица ковариаций~\eqref{ch5:eq:covMatrix} множества объектов $\{\hat{\bx}_i\}_{i=1}^m$
\[
	\hat{\mathbf{A}} =
	\frac{1}{m} \sum_{i=1}^m (\bt_i - \hat{\bc})(\bt_i - \hat{\bc})^{\T} =
	\frac{1}{m} \sum_{i=1}^m \bP(\bx_i - \bc)(\bx_i - \bc)^{\T} \bP^{\T} =  \bP \mathbf{A} \bP^{\T}.
\]
\begin{definition}
	\textit{Индикаторной матрицей} назовем матрицу $\bY \in \bbR^{m \times K}$, где
	$y_{ij} =[f(\bx_i) = y_j]$.
\end{definition}
\begin{definition}
	\textit{Взвешенной индикаторной матрицей} назовем матрицу
	$\mathbf{L} = \bY (\bY^{\T} \bY)^{-1/2} \in \bbR^{m \times K}$, элементы которой равны:
	\[
		l_{ij} =
		\begin{cases}
			\displaystyle    \frac{1}{\sqrt{N_j}}, & \text{если $f(\bx_i) = y_j$;} \\
			0, & \text{если $f(\bx_i) \neq y_j$.}
		\end{cases}
	\]
\end{definition}
Задача кластеризации~\eqref{ch5:eq:Qmax} и~задача метрического обучения~\eqref{ch5:eq:Amax} сводятся к общей задаче минимизации функции ошибки
\begin{multline}
	\cL = -\frac{1}{m} \text{trace} (\mathbf{L}^{\T} \bX^{\T} \bP^{\T} \hat{\mathbf{A}}^{-1} \bP \bX \mathbf{L}) = \\ = - \frac{1}{m} \text{trace} (\mathbf{L}^{\T} \bX^{\T} \bP^{\T}
	(\bP \mathbf{A} \bP^{\T})^{-1} \bP \bX \mathbf{L}) \to \min_{\bP, \mathbf{L}}.
	\label{ch5:eq:GLmax}
\end{multline}

Для решения задачи~\eqref{ch5:eq:GLmax} используется EM алгоритм.

\textbf{Оптимизация матрицы P с фиксированной матрицей L.}
Переформулируем задачу~\eqref{ch5:eq:GLmax} следующим образом:
\[
	\cL = -\frac{1}{m} \text{trace} (\mathbf{L}^{\T} \bX^{\T} \bP^{\T} (\bP \mathbf{A} \bP^{\T})^{-1} \bP \bX \mathbf{L}) = -\frac{1}{m} \text{trace} \bigl((\bP \mathbf{A} \bP^{\T})^{-1} \bP \bX \mathbf{LL}^{T} \bX^{\T} \bP^{\T}\bigr).
\]
\begin{statement}
	Обозначим $\mathbf{B} = \bX \mathbf{LL}^{\T} \bX^{\T}$.
	Обозначим через $\bP = [\mathbf{v}_1, \ldots, \mathbf{v}_r]^{\bT}$ матрицу, состоящую из $r$ собственных векторов матрицы $\mathbf{A}^{-1}\mathbf{B}$, отвечающих наибольшим собственным значениям.
	Тогда решением~\eqref{ch5:eq:GLmax} является ортогональная матрица, полученная QR-разложением матрицы~$\bP^{\T}$.
\end{statement}

\textbf{Оптимизация матрицы L с фиксированной матрицей P.}
Обозначим $\hat{\mathbf{K}} = (1/N)\bX^{\T} \bP^{\T} \hat{\mathbf{A}}^{-1} \bP \bX$.

При фиксированной матрице $\bP$ задача~\eqref{ch5:eq:GLmax} принимает вид:
\begin{equation*}
	\text{trace} (\mathbf{L}^{\top} \hat{\mathbf{K}} \mathbf{L}) \to \max_{\mathbf{L} \in \bR^{m \times r}}.
\end{equation*}

Для нахождения оптимального соответствия между временными рядами при решении задачи классификации предлагается процедура выравнивания временных рядов.
Пусть задана выборка $\{(\mathbf{x}_i, y_i)\}_{i=1}^m$~--- множество объектов с известными метками классов $y_i \in \bbY$, где $\bbY = \{1, \dots, K\}$~--- множество меток классов.

Требуется построить точную, простую, устойчивую модель классификации $a: \bbR^n \to \bbY$.
Данную модель представим в виде суперпозиции
\begin{equation*}
	a(\mathbf{x}) = b \circ \mathbf{f} \circ G\bigl(\mathbf{x}, \{\mathbf{c}_e\}_{e = 1} ^ K\bigr),
\end{equation*}
где $G$~--- процедура выравнивания временных рядов относительно центроидов классов~$\{\mathbf{c}_e\}_{e = 1} ^ K$, $\mathbf{f}$~--- алгоритм метрического обучения, $b$~--- алгоритм многоклассовой классификации.

Для нахождения центроида предлагается в качестве расстояния $\rho$ между временными рядами использовать путь наименьшей стоимости, найденный методом динамической трансформации времени.

\textbf{Метрическое обучение.}
Введем на множестве выравненных временных рядов расстояние Махаланобиса $d_{\bA}$~\ref{ch5:eq:mahal_distance}.
Представим матрицу трансформации~$\bA$ в виде разложения $\mathbf{A}^{-1} = \mathbf{L}^{\T}  \mathbf{L}$.

Расстояние $d_\mathbf{A} (\mathbf{x}_i, \mathbf{x}_j)$ есть евклидово расстояние между $\mathbf{Lx}_i$ и $\mathbf{Lx}_j$:
\begin{multline*}
	d_\mathbf{A} (\mathbf{x}_i, \mathbf{x}_j) = \sqrt{(\mathbf{x}_i - \mathbf{x}_j)^{\T} \mathbf{L}^{\T} \mathbf{L} (\mathbf{x}_i - \mathbf{x}_j)} = \\= \sqrt{(\mathbf{L} (\mathbf{x}_i - \mathbf{x}_j))^{\T} (\mathbf{L} (\mathbf{x}_i - \mathbf{x}_j))} = \|\mathbf{L} (\mathbf{x}_i - \mathbf{x}_j)\|_2.
\end{multline*}

В качестве алгоритма метрического обучения в данной работе был выбран алгоритм LMNN. 
Данный алгоритм сочетает в себе идеи метода $k$ ближайших соседей. 
Первая идея заключается в минимизации расстояний между $k$ ближайшими объектами, находящимися в одном классе:
\[
	Q_1(\mathbf{L}) = \sum_{j \rightsquigarrow i} \|\mathbf{L}(\mathbf{x}_i - \mathbf{x}_j)\|^2 \rightarrow \min_{\mathbf{L}},
\]
где $j \rightsquigarrow i$ означает, что $\mathbf{x}_j$ является одним из $k$ ближайших соседей для $\mathbf{x}_i$.
Вторая идея состоит в максимизации расстояния между каждым объектом и его объектами-нарушителями. 
\begin{definition}
	\textit{Объектом-нарушителем} для $\mathbf{x}_i$ назовем объект~$\mathbf{x}_l$ такой, что
	\begin{equation}
		\label{ch5:eq:impostor}
		\|\mathbf{L}(\mathbf{x}_i - \mathbf{x}_l)\|^2 \leq \|\mathbf{L}(\mathbf{x}_i - \mathbf{x}_j)\|^2 + 1, \quad \text{где $j \rightsquigarrow i$}.
	\end{equation}
\end{definition}
Таким образом, необходимо минимизировать следующий функционал:
\[
	Q_2(\mathbf{L}) = \sum_{j \rightsquigarrow i} \sum_l [y_i \neq y_l] \bigl[1 + \|\mathbf{L}(\mathbf{x}_i - \mathbf{x}_j)\|^2 - \|\mathbf{L}(\mathbf{x}_i - \mathbf{x}_l)\|^2\bigr]_+ \rightarrow \min_{\mathbf{L}}.
\]

Задача метрического обучения состоит в нахождении линейного преобразования $\mathbf{f}(\mathbf{x}) = \mathbf{Lx}$, то есть нахождении матрицы $\mathbf{L}$ в виде решения оптимизационной задачи
\begin{equation}
	\label{ch5:eq:Qmin}
	Q(\mathbf{L}) = \mu Q_1(\mathbf{L}) + (1 - \mu) Q_2(\mathbf{L}) \rightarrow \min_{\mathbf{L}}.
\end{equation}

В \textbf{главе 6} ставится задача порождения информативного признакового пространства.
Временные ряды акселерометра образуют множество~$\mathcal{S}$ сегментов~$\bs = [x_1, \dots, x_T]^{\T}$ фиксированной длины~$T$.
Необходимо построить модель классификации~$a: \bbR^T \rightarrow \bbY$, которая будет ставить в соответствие каждому сегменту из множества~$\mathcal{S}$ метку класса из конечного множества~$\bbY = \{1, \dots, K\}$.
Обозначим за $\mathcal{D} = \{(\bs_i, y_i)\}_{i=1}^m$ исходную выборку, где $\bs_i \in \mathcal{S}$ и $y_i = a(\bs_i)\in \bbY$.

В работе предлагается построить модель~$a$ в виде суперпозиции $a = f \circ \mathbf{g}$.
\vspace{0.1cm}
\begin{definition}
	\textit{Порождающей функцией} будем называть функцию $\mathbf{g}: \bbR^T \rightarrow \mathbb{X}$, отображающую исходные временные ряды $\bs$ из пространства~$\bbR^{T} $ в признаковое пространство~$\bbX \subset \bbR^n$.
\end{definition}
Имея порождающую функцию~$\mathbf{g}$, преобразуем исходную выборку в $\mathcal{D}_{\bbX} = \{(\bx_i, y_i)\}_{i=1}^m$, где $\bx_i = \mathbf{g}(\bs_i) \in \bbX$. Здесь $\bx_i$ является исходным объектом.

Модель классификации $f=f(\bx, \btheta)$ является параметрической функцией с вектором параметров~$\btheta$. 
Оптимальные параметры~$\btheta^*$ определяются оптимизацией функции ошибки классификации
\begin{equation}
	\btheta^* = \argmin_{\btheta} L(\btheta, \mathcal{D}_\bbX, \bmu).
	\label{ch6:eq:optimal_classification_params}
\end{equation}

\textbf{Экспертные функции.}
Экспертные функции~--- это некоторые статистики~$g_j$, где $g_j: \bbR^T \rightarrow \bbR$.
Признаковым описанием~$\mathbf{g}(\bs)$ объекта~$s$ являются значения заданных экспертных статистик для данного объекта
\[
	\bx = \mathbf{g}(\bs) = [g_1(\bs), \dots, g_n(\bs)]^{\T}.
\]

\textbf{Авторегрессионная модель.}
Авторегрессионная модель порядка~$n$ использует параметрическую модель для аппроксимации временного ряда~$\bs$:
\begin{equation*}
	x_t = w_0 + \sum_{j=1}^{n-1} w_j x_{t-j} + \epsilon_t.
\end{equation*}
Оптимальные параметры $\mathbf{w}^*$ авторегрессионной модели используются как признаки $\mathbf{g}(\bs)$:
\begin{equation}
	\bx = \mathbf{g}(\bs) = \mathbf{w}^* = \argmin_{\mathbf{w} \in \bbR^{n}} \left( \sum_{t=n}^{T} \|x_t - \hat{x}_t\|^2\right).
	\label{ch6:eq:autoregressive_description}
\end{equation}

\textbf{Анализ сингулярного спектра.}
Для каждого временного ряда~$\bs$ из исходной выборки~$\mathcal{D}$ строится траекторная матрица:
\[
	\bS = 
	\begin{pmatrix}
		s_1 & s_2 & \dots & s_n \\
		s_2 & s_3 & \dots & s_{n+1} \\
		\dots & \dots & \dots & \dots \\
		s_{T-n+1} & s_{T-n+2} & \dots & s_T
	\end{pmatrix}.
\]
Сингулярное разложение матрицы $\mathbf{S}^{\T} \mathbf{S} = \bU \boldsymbol{\Lambda} \bU^{\T}$.
Признаковое описание объекта~$\bs$ задаётся спектром матрицы~$\bS^{\T} \bS$:
\[
	\bx = \mathbf{g}(\bs) = \left[\lambda_1, \dots, \lambda_n\right]^{\T}.
\]

\textbf{Аппроксимация сплайнами.}
Предполагается, что узлы сплайна $\{\xi_\ell\}_{\ell=0}^M$ равномерно распределены по временной оси.
Кусочные модели, построенные на отрезках $[\xi_{\ell-1}; \xi_{\ell}]$, заданы коэффициентами $\{\mathbf{w}_\ell\}_{\ell=1}^{M}$.
Обозначим каждый отрезок-сегмент $p_i(t)$ $i = 1, \dots, M$ и весь сплайн $S(t)$:
\begin{equation*}
	S(t) = \begin{cases}
		p_1(t) = w_{10} +w_{11}t + w_{12}t^2 + w_{13}t^3, & t\in [\xi_0, \xi_1],\\
		p_2(t) = w_{20} +w_{21}t + w_{22}t^2 + w_{23}t^3, & t\in [\xi_1, \xi_2],\\
		\cdots&\cdots \\
		p_{M}(t) = w_{L0} +w_{M1}t + w_{M2}t^2 + w_{M3}t^3, & t\in [\xi_{M-1}, \xi_M],					
	\end{cases}
\end{equation*}
\begin{align*}
	S(\xi_t) &= x_t, \quad t = 0, \dots, M,\\
	p_i'(\xi_i) &= p_{i+1}'(\xi_i),\: p_i''(\xi_i) = p_{i+1}''(\xi_i), \quad i = 1, \dots, M-1,\\
	p_i(\xi_{i-1}) &= x_{i-1},\: p_i(\xi_i) = x_i, \quad i = 1, \dots, M.
\end{align*}
Объединение всех параметров сплайна задаёт признаковое описание временного ряда:
\[
	\bx = \mathbf{g}(\bs) = \left[\mathbf{w}_1, \dots, \mathbf{w}_{M}\right]^{\T}.
\]

\textbf{В заключении} представлены основные результаты диссертационной работы.

\begin{enumerate}
	\item Исследована проблема снижения размерности сигналов в коррелированных пространствах высокой размерности. 
	Предложены методы декодирования сигналов, учитывающие зависимости как в исходном, так и в целевом пространстве сигналов.
	\item Доказаны теоремы об оптимальности предлагаемых методов декодирования сигналов. Предлагаемые методы выбирают согласованные модели в случае избыточной размерности описания данных.
	\item Предложены методы выбора признаков, учитывающие зависимости как в исходном, так и в целевом пространстве. Предложенные методы доставляют устойчивые и адекватные решения в пространствах высокой размерности.
	\item Предложены нелинейные методы согласования скрытых пространств для данных со сложноорганизованной целевой переменной.
	\item Предложен ряд моделей для прогнозирования гетерогенных наборов сигналов для задачи построения нейрокомпьютерных интерфейсов.
\end{enumerate}

\subsection*{Публикации соискателя по теме диссертации}
Публикации в журналах из списка ВАК.
\vspace{0.3cm}
\begin{enumerate}
	\item Исаченко Р. В., Стрижов В. В. Метрическое обучение в задачах мультиклассовой классификации временных рядов // Информатика и её применения, 2016. Т. 10. № 2. С. 48--57.
	\item Isachenko R. et al. Feature Generation for Physical Activity Classification // Artificial Intelligence and Decision Making, 2018. № 3. С. 20--27.
	\item Isachenko R. V., Strijov V. V. Quadratic programming optimization with feature selection for nonlinear models // Lobachevskii Journal of Mathematics, 2018. Т. 39. № 9. С. 1179--1187.
	\item Isachenko R. V., Vladimirova M. R., Strijov V. V. Dimensionality Reduction for Time Series Decoding and Forecasting Problems //DEStech Transactions on Computer Science and Engineering, 2018. №. optim.
	\item Исаченко Р.В., Яушев Ф.Ю., Стрижов В.В. Модели согласования скрытого пространства в задаче прогнозирования // Системы и средства информатики, 2021. Т. 31. № 1.
\end{enumerate}

\vspace{0.4cm}
{Прочие публикации.}
\vspace{0.2cm}
\begin{enumerate}
\setcounter{enumi}{5}
	\item Исаченко Р. В., Катруца А. М. Метрическое обучение и снижение размерности пространства в задачах кластеризации // Машинное обучение и анализ данных, 2016. T. 2. № 1. С. 17--25.
\end{enumerate}

\end{document}