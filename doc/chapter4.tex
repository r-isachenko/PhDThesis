\chapter{Выбор параметров нелинейных моделей с помощью квадратичного отбора признаков}
\label{ch:newton_qpfs}

Функция ошибки для моделей с большим числом параметров имеет сложный ландшафт с многими локальными минимумами.
В этом случае алгоритм оптимизации приводит к разным решениям в зависимости от инициализации исходных параметров.

Алгоритм оптимизации представляет собой итерационный процесс.
На каждом шаге для получения следующего приближения параметров модели обновляются текущие параметры.
Разработано множество алгоритмов оптимизации первого порядка, использующих вектор первых производных функции ошибки.
Наиболее известными алгоритмами являются градиентный спуск, 
метод момента Нестерова~\cite{nesterov1983momentum}, AdaGrad~\cite{duchi2011adagrad}, Adam~\cite{kingma2014adam}.
Данные алгоритмы используются для оптимизации глубоких нейронных сетей~\cite{goodfellow2016deeplearningbook}.
Метод Ньютона~--- алгоритм второго порядка, использующий матрицу вторых производных функции ошибки.
Метод Ньютона находит обновления параметров для квадратичной аппроксимации функции ошибки и сходится за адекватное число итераций.
Недостатком методов оптимизации второго порядка является огромная и плохо обусловленная матрица Гессиана.
Процесс оптимизации в этом случае расходится и является вычислительно дорогостоящим.
Авторы~\cite{avriel2003nonlinear,blaschke1997convergence} предлагают аппроксимации для матрицы Гессиана и регуляризацию для решения этой проблемы.
В статье~\cite{botev2017newtondeeplearning} метод Ньютона применяется к глубоким нейронным сетям.

В данной главе приводится анализ параметров модели, которые не находятся в оптимуме.
Приводится метод выбора активных параметров модели, основанный на методе QPFS, который подробно описан в главе~\ref{ch:qpfs}.
Рассматриваются задачи нелинейной регрессии с квадратичной функцией потерь, логистической регрессии с кросс-энтропийной функцией потерь.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Задача выбора параметров для оптимизации нелинейных моделей}
\label{sec:ch4:newton_qpfs_param_selection}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Модель $f( \bx, \btheta)$ с параметрами $\btheta \in \mathbb{R}^p$ предсказывает целевой объект~$y \in \bbY$ по исходному объекту~$\bx \in \bbR^{n}$. Пространство~$\bbY$ представляет собой бинарные метки классов~$\{0, 1\}$ для задачи двухклассовой классификации и~$\bbR$ для задачи регрессии.
Даны матрица исходных объектов~$\bX = [\bx_1, \dots, \bx_m]^{\T} \in \bbR^{m \times n}$ и целевой вектор~$\by = [y_1, \dots, y_m]^{\T} \in \bbY^{m}$. 
Цель состоит в нахождении оптимальных параметров~$\btheta^*$.
Параметры~$\btheta$ вычисляются минимизацией функции ошибки:
\begin{equation}
	\btheta^* = \argmin_{\btheta \in \bbR^p} \cL(\btheta, \bX, \by).
	\label{ch4:eq:error_function}
\end{equation}
Данная задача полностью соответствует рассмотренной задаче~\eqref{ch1:eq:loss_min_param} для случая скалярной целевой переменной ($r=1$).
В качестве функции ошибки~$\cL (\btheta, \bX, \by)$ рассматриваются квадратичная ошибка для задачи регрессии:
\begin{equation}
	\cL(\btheta, \bX, \by) = \frac 12 \| \by - \mathbf{f}(\bX, \btheta) \|^2 = \frac 12 \sum_{i=1}^m \bigl( y_i - f(\bx_i,  \btheta)\bigr)^2,
	\label{ch4:eq:squared_error}
\end{equation}
и функция кросс-энтропии для задачи бинарной классификации: 
\begin{equation}
	\cL(\btheta, \bX, \by) = \sum_{i=1}^m \bigl[y_i \log f (\bx_i , \btheta) + (1-y_i) \log \bigl(1 - f (\bx_i , \btheta)\bigr)\bigr].
	\label{ch4:eq:log_loss}
\end{equation}

Задача~\eqref{ch4:eq:error_function} решается с помощью итеративной процедуры оптимизации. 
Для получения параметров на шаге~$k$ текущие параметры $\btheta^{k-1}$ обновляются по следующему правилу:
\begin{equation}
	\btheta^k = \btheta^{k - 1} + \Delta \btheta^{k - 1}.
	\label{ch4:eq:update_rule}
\end{equation}
Для выбора вектора обновлений~$\Delta \btheta$ используется метод оптимизации Ньютона.

Метод Ньютона нестабилен и вычислительно сложен. 
В работе предлагается стабильный метод Ньютона. 
Перед шагом градиента предлагается выбрать подмножество активных параметров модели, которые оказывают наибольшее влияние на функцию ошибки~$\cL (\btheta, \bX, \by)$.
Введём определение активного параметра модели, используя необходимое условие оптимальности первого порядка.
\begin{definition}
	\label{ch4:def:active_param}
	Параметр $\theta_j$ для модели $f(\bx, \btheta)$ является \textit{активным}, если $\bJ^{\T} (\mathbf{f}(\bx, \btheta) - \by) \neq 0$.
\end{definition}
Подробный вывод условия из определения приводится в разделе~\ref{sec:ch4:newton_qpfs_algorithm}.
Обновление параметров производится только для отобранного множества индексов~$\cA = \bigl\{j: a_j = 1, \ba \in \{0, 1\}^p\bigr\}$
\begin{align*}
	\btheta_{\cA}^k &= \btheta_{\cA}^{k - 1} + \Delta \btheta_{\cA}^{k - 1}, \quad \btheta_{\cA} = \{\theta_j: j \in \cA \}, \\
	\btheta_{\bar{\cA}}^k &= \btheta_{\bar{\cA}}^{k - 1}, \quad \btheta_{\bar{\cA}} = \{\theta_j: j \notin \cA \}.
\end{align*}
Чтобы выбрать оптимальное подмножество индексов~$\cA$, из всех возможных $2^p - 1$~подмножеств, вводится функция ошибки
\begin{equation}
	\ba = \argmin_{\ba' \in \{0, 1\}^p} S(\ba', \bX, \by, \btheta),
	\label{ch4:eq:subset_selection}
\end{equation}
аналогичная функции ошибки~\eqref{ch3:eq:feature_selection} для задачи выбора признаков. 
Задача~\eqref{ch4:eq:subset_selection} решается на каждом шаге $k$ процесса оптимизации для текущих параметров~$\btheta^k$.

Метод QPFS используется для решения задачи~\eqref{ch4:eq:subset_selection}.
QPFS выбирает подмножество параметров~$\ba$ для вектора обновлений~$ \Delta \btheta$, которые оказывают наибольшее влияние на вектор остатков и являются попарно независимыми.
Функция ошибки~\eqref{ch3:eq:qpfs_problem} соответствует функции ошибки~$S(\ba, \bX, \by, \btheta)$
\begin{equation}
	\ba = \argmax_{\ba' \in \{1, 0\}^p} S(\ba', \bX, \by, \btheta) \Leftrightarrow \argmin_{\ba  \in \bbR^p_+, \, \|\ba\|_1=1} \bigl[\ba^{\T} \bQ \ba - \alpha \cdot \mathbf{b}^{\T} \ba \bigr].
\end{equation}
В работе показано, что для модели нелинейной регрессии с квадратичной функцией ошибки~\eqref{ch4:eq:squared_error} и для модели логистической регрессии с кросс-энтропией~\eqref{ch4:eq:log_loss}, каждый шаг оптимизации эквивалентен задаче линейной регрессии~\eqref{ch3:eq:linear_regression}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Метод Ньютона для оптимизации параметров}
\label{sec:ch4:newton_algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Метод Ньютона использует условие оптимизации первого порядка для задачи~\eqref{ch4:eq:error_function} и линеаризует градиент $S (\btheta)$:
\[
	\nabla S (\btheta + \Delta \btheta) = \nabla S(\btheta) + \bH \cdot \Delta \btheta = \bZero,
\]
\[
	\Delta \btheta = - \bH^{-1} \nabla S(\btheta).
\]
где $\bH = \nabla^2 S(\btheta)$ является матрицой Гессиана функции ошибки $S(\btheta)$.

Итерация~\eqref{ch4:eq:update_rule} метода Ньютона имеет вид
\begin{equation}
	\btheta^k = \btheta^{k-1} - \bH^{-1} \nabla S(\btheta).
	\label{ch4:eq:newton_step}
\end{equation}
На каждой итерации требуется обращать матрицу Гессиана $\bH$.
Мерой плохой обусловленности для матрицы Гессиана~$\bH$ является число обусловленности
\[
	\kappa(\bH) = \frac{\lambda_{\text{max}}(\bH)}{\lambda_{\text{min}}(\bH)},
\]
где $\lambda_{\text{max}}(\bH), \lambda_{\text{min}}(\bH)$ являются максимальным и минимальным собственными значениями~$\bH$. Большое число обусловленности~$\kappa (\bH)$ приводит к нестабильности процесса оптимизации.
Предложенный метод уменьшает размер матрицы Гессиана~$\bH$. Согласно экспериментам, приведенным в разделе~\ref{sec:ch4:newton_qpfs_exp} предлагаемый метод приводит к меньшему числу обусловленности~$\kappa (\bH)$.

Размер шага метода Ньютона может быть чрезмерно большим. Для контроля размера шага обновлений добавим параметр $\eta$ в правило обновления~\eqref{ch4:eq:update_rule}
\[
	\btheta^k = \btheta^{k - 1} + \eta \Delta \btheta^{k - 1}, \quad \eta \in [0, 1].
\]

Для выбора соответствующего размера шага~$\eta$ используется правило Армихо~\cite{armijo1966minimization}. Выбирается максимальное~$\eta$ так, чтобы выполнялось условие
\[
	S(\btheta^{k - 1} + \eta \Delta \btheta^{k - 1}) < S(\btheta^{k - 1}) + \gamma \eta \nabla S^{\T}(\btheta^{k-1})\btheta^{k - 1}, \quad \gamma \in [0, 0.5].
\]

\begin{theorem}
	Пусть модель $f (\bx , \btheta)$ близка к линейной в окрестности точки $\btheta + \Delta \btheta$
	\begin{equation}
		\mathbf{f}(\bX , \btheta + \Delta \btheta) \approx \mathbf{f}(\bX , \btheta) + \bJ \cdot \Delta  \btheta,
		\label{ch4:eq:linearization}
	\end{equation}
	где $\mathbf{J} \in \bbR^{m \times p}$ является матрицой Якоби
	\begin{equation*}
		\bJ = 
		\begin{pmatrix}
			\frac{\partial f(\bx_1 , \btheta)}{\partial \theta_1} & \dots & 
			\frac{\partial f(\bx_1 , \btheta)}{\partial \theta_p} \\
			\dots & \dots & \dots \\
			\frac{\partial f(\bx_m , \btheta)}{\partial \theta_1} & \dots & 
			\frac{\partial f(\bx_m , \btheta)}{\partial \theta_p}
		\end{pmatrix}.
	\end{equation*}
	Тогда вектор обновления~$\Delta \btheta$ для функции ошибки~\eqref{ch4:eq:squared_error} является решением задачи линейной регрессии
	\begin{equation}
		\| \be - \bF \Delta \btheta \|_2^2 \rightarrow \min_{\Delta \btheta \in \bbR^{p}},
		\label{ch4:eq:lin_reg_nonlin_reg}
	\end{equation}
	где $\be = \mathbf{f} - \by$ и $\bF = \bJ$.
\end{theorem}
\begin{proof}
	В соответствии предположением~\eqref{ch4:eq:linearization} градиент~$\nabla S(\btheta)$ и матрица Гессиана~$\bH$ имеют вид
	\begin{equation}
		\nabla S(\btheta) = \bJ^{\T} (\by - \mathbf{f}), \quad \bH = \bJ^{\T} \bJ.
		\label{ch4:eq:nonlin_reg_deriv}
	\end{equation}
	Тогда шаг метода Ньютона~\eqref{ch4:eq:newton_step} и правило обновления~\eqref{ch4:eq:update_rule} принимают вид
	\[
		\btheta^k = \btheta^{k - 1} + \Delta \btheta^{k - 1} = \btheta^{k - 1} + (\bJ^{\T} \bJ)^{-1}\bJ^{\T}(\mathbf{f} - \by).
	\]
	Таким образом, согласно теореме Гаусса-Маркова, вектор обновления~$\Delta \btheta$ является решением задачи регрессии~\eqref{ch4:eq:lin_reg_nonlin_reg}.
\end{proof}
В качестве нелинейной модели рассматривается модель двухслойной нейронной сети. В этом случае модель~$f (\bx, \btheta)$ принимает вид:
\[
	f(\bx, \btheta) = \sigma(\bx^{\T} \bW_1) \bw_2.
\]
Здесь~$\bW_1 \in \bbR^{m \times h}$~--- матрица параметров, которые соединяют исходные признаки с $h$ скрытыми нейронами. Функция нелинейности $\sigma(\cdot)$ применяется поэлементно. Параметры~$\bw_2 \in \bbR^{h \times 1}$ соединяют скрытые нейроны с выходом. 
Вектор параметров модели~$\btheta$ представляет собой объединение векторизованных матриц~$\bW_1$, $\bw_2$.

\begin{theorem}
Рассмотрим модель логистической регрессии вида $f(\bx , \btheta) = \sigma(\bx^{\T} \btheta)$ с сигмоидной функцией активации~$\sigma(\cdot)$. 
Вектор обновлений $\Delta \btheta$ для функции ошибки~\eqref{ch4:eq:log_loss} является решением задачи линейной регрессии
\begin{equation}
	\| \be - \bF \Delta \btheta \|_2^2 \rightarrow \min_{\Delta \btheta \in \bbR^{p}},
	\label{ch4:eq:lin_reg_log_reg}
\end{equation}
где $\be = \bR^{-1/2} (\by - \mathbf{f})$ и $\bF = \bR^{1/2}\bX$.
\end{theorem}
\begin{proof}
	Градиент и Гессиан функции ошибки~\eqref{ch4:eq:log_loss} равны
	\begin{equation}
		\nabla S(\btheta) = \bX^{\T} (\mathbf{f} - \by), \quad \bH = \bX^{\T} \bR \bX,
		\label{ch4:eq:log_reg_deriv}
	\end{equation}
	где $\bR$~--- это диагональная матрица с диагональными элементами $f(\bx_i , \btheta) \cdot (1 - f(\bx_i , \btheta))$.
	
	Правило обновления~\eqref{ch4:eq:update_rule} в этом случае принимает вид
	\[
		\btheta^k = \btheta^{k - 1} + (\bX^{\T} \bR \bX)^{-1} \bX^{\T} (\by - \mathbf{f}).
	\]
	Таким образом, согласно теореме Гаусса-Маркова, вектор обновления~$\Delta \btheta$ является решением задачи регрессии~\eqref{ch4:eq:lin_reg_nonlin_reg}.
\end{proof}
Данный алгоритм известен как итеративный алгоритм взвешенных наименьших квадратов (IRLS)~\cite{holland1977robust}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Метод Ньютона с выбором параметров с помощью квадратичного программирования}
\label{sec:ch4:newton_qpfs_algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Предлагается адаптация метода QPFS для решения задач~\eqref{ch4:eq:lin_reg_nonlin_reg} и \eqref{ch4:eq:lin_reg_log_reg}. 
Матрица парных взаимодействий~$\bQ$ и вектор релевантностей~$\bb$ имеют вид
\[
	\bQ = \text{Sim} (\bF), \quad \bb = \text{Rel} (\bF, \be).
\]

\begin{statement}
	В оптимальной точке~$\btheta^*$ вектор релевантностей~$\bb = \text{Rel} (\bF, \be)$ равен нулю.
\end{statement}
\begin{proof}
	Выборочный коэффициент корреляции равен нулю для ортогональных векторов.
	Покажем, что в оптимальной точке~$\btheta^*$ вектор~$\be$ ортогонален столбцам матрицы~$\bF$. 
	Условие оптимальности первого порядка гарантирует это свойство для модели нелинейной регрессии
	\[
		\bF^{\T} \be = \bJ^{\T} (\mathbf{f} - \by) = - \nabla S(\btheta^*) = \boldsymbol{0},
	\]
	и для модели логистической регрессии
	\[
		\bF^{\T} \be = \bX \bR^{-1/2} \bR^{1/2} (\by - \mathbf{f}) = \bX^{\T} (\by - \mathbf{f}) = \nabla S(\btheta^*) = \boldsymbol{0}.
	\]
\end{proof}
Данное утверждение используется в качестве индикатора активности параметра модели в определении~\ref{ch4:def:active_param}.
Псевдокод предлагаемого алгоритма приведён в алгоритме~\ref{ch4:alg:QPFSNewton}.

\begin{algorithm}[ht]
	\caption{QPFS + Ньютон алгоритм}
	\label{ch4:alg:QPFSNewton}
	\begin{algorithmic}
		\REQUIRE $\varepsilon$~--- допустимое отклонение;\\
		\hspace{1.07cm}$\tau$~-- пороговое значение;\\
		\hspace{1.07cm}$\gamma$~--- параметр правила Армихо.
		\ENSURE $\btheta^*$;
		\STATE  инициализировать $\btheta^0$;
		\STATE $k := 1$;
		\REPEAT
		\STATE вычислить $\be$ и $\bF$ для~\eqref{ch4:eq:lin_reg_nonlin_reg} или~\eqref{ch4:eq:lin_reg_log_reg} ;
		\vspace{0.1cm}
		\STATE $\bQ := \text{Sim} (\bF)$, $\bb := \text{Rel}(\bF, \be)$, $\alpha = \frac{\overline{\bQ}}{\overline{\bQ} + \overline{\bb}}$;
		\vspace{0.1cm}
		\STATE $\ba := \argmin_{\ba \geq 0, \, \|\ba\|_1=1}\ba^{\T} \bQ \ba - \alpha \cdot \mathbf{b}^{\T} \ba$;
		\vspace{0.1cm}
		\STATE $\cA := \{j: a_j = 1\}$;
		\vspace{0.1cm}
		\STATE вычислить $\nabla S(\btheta^{k-1})$, $\bH$ для \eqref{ch4:eq:nonlin_reg_deriv} или \eqref{ch4:eq:log_reg_deriv};
		\vspace{0.1cm}
		\STATE $\Delta \btheta^{k-1} = - \bH^{-1} \nabla S(\btheta^{k-1})$;
		\vspace{0.1cm}
		\STATE $\eta := \text{ArmijoRule}(\btheta^{k-1}, \gamma)$;
		\vspace{0.1cm}
		\STATE $\btheta_{\cA}^k = \btheta_{\cA}^{k - 1} + \eta \Delta \btheta_{\cA}^{k - 1}$;
		\vspace{0.1cm}
		\STATE $k := k + 1$;
		\vspace{0.1cm}
		\UNTIL{$\frac{\| \btheta^k - \btheta^{k-1} \|}{\| \btheta^k \|} < \varepsilon$}
	\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Анализ значимостей параметров нелинейных моделей}
\label{sec:ch4:newton_qpfs_exp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Целью вычислительного эксперимента является исследование свойств предложенного метода и сравнение его с другими методами. 

Исследована зависимость параметров метода QPFS для задачи нелинейной регрессии~\eqref{ch4:eq:lin_reg_nonlin_reg} и задачи логистической регрессии~\eqref{ch4:eq:lin_reg_log_reg}. 
Предположим, что вектор параметров~$\btheta^0$ лежит вблизи оптимального вектора параметров~$\btheta^*$. 
Рассмотрим отрезок
\[
	\btheta_{\beta} = \beta \btheta^* + (1 - \beta) \btheta^0; \quad \beta \in [0, 1] .
\]

Сгенерируем синтетический набор данных с 300 объектами и 7 признаками для задачи логистической регрессии. 
Ландшафт функции ошибки~\eqref{ch4:eq:log_loss} на сетке двух случайно выбранных параметров показан на Рис.~\ref{ch4:fig:log_reg_error}.
Поверхность функции ошибки выпуклая с вытянутыми линиями уровня вдоль некоторых параметров модели.
Добавим случайный шум к оптимальным параметрам~$\btheta^*$, чтобы получить точку~$\btheta^0$. Поведение вектора~$\bb$ на отрезке между~$\btheta^0$ и~$\btheta^*$ показано на Рис.~\ref{ch4:fig:log_reg_b_wrt_beta}.
Компоненты~$\bb$ начинают резко уменьшаться по мере приближения к оптимальной точке $\btheta^*$.
\begin{figure}[h]
	\centering
	\begin{minipage}{.47\textwidth}
		\centering
		\includegraphics[width=\linewidth]{figs/ch4/log_reg_error}
		\caption{Поверхность функции ошибки для логистической регрессии}
		\label{ch4:fig:log_reg_error}
	\end{minipage}%
	\begin{minipage}{.47\textwidth}
		\centering
		\includegraphics[width=\linewidth]{figs/ch4/log_reg_b_wrt_beta}
		\caption{Релевантность параметров для логистической регрессии}
		\label{ch4:fig:log_reg_b_wrt_beta}
	\end{minipage}
\end{figure}

Для модели нелинейной регрессии используется классический набор данных Boston Housing с 506 объектами и 13 признаками.
Для простоты нейронная сеть содержит два скрытых нейрона.
Ландшафт функции ошибок для модели нейронной сети является более сложным. 
Функция ошибки не является выпуклой и содержит множество локальных минимумов.
Двумерный ландшафт функции ошибок для этого набора данных показан на Рис.~\ref{ch4:fig:neural_error}. 
Сетка строится для двух случайных параметров из матрицы~$\bW_1$.
Аналогично на Рис.~\ref{ch4:fig:neural_b_wrt_beta} показано, как изменяется вектор~$\bb$ при движении от точки~$\btheta^0$ до точки~$\btheta^*$. 
Компоненты вектора $\bb$ становятся близки к нулю вблизи оптимума. 
При достижении оптимального значения различные параметры влияют на остатки модели~$\be$.
\begin{figure}[h]
	\centering
	\begin{minipage}{.5\textwidth}
		\centering
		\includegraphics[width=\linewidth]{figs/ch4/neural_error}
		\caption{Поверхность функции ошибки для нейронной сети}
		\label{ch4:fig:neural_error}
	\end{minipage}%
	\begin{minipage}{.5\textwidth}
		\centering
		\includegraphics[width=\linewidth]{figs/ch4/neural_b_wrt_beta}
		\caption{Релевантность параметров первого слоя для модели нейронной сети}
		\label{ch4:fig:neural_b_wrt_beta}
	\end{minipage}
\end{figure}

На Рис.~\ref{ch4:fig:irls_qpfs_2d} показан процесс оптимизации для предложенного метода в случае логистической регрессии с двумя параметрами модели. 
Даже для двумерной задачи решение метода Ньютона нестабильно и число обусловленности $\kappa(\bH)$ матрицы Гессиана~$\bH$ может быть чрезвычайно большим. 
На каждом шаге алгоритма метод QPFS выбирает активные параметры для оптимизации. 
В данном примере предложенный метод выбирает и обновляет только один параметр на каждой итерации на первых шагах. 
Это делает метод более устойчивым.

\begin{figure}
	\centering
	\includegraphics[width=0.6\linewidth]{figs/ch4/irls_qpfs_2d}	 
	\caption{Оптимизационный процесс предложенного метода QPFS+Ньютон для модели логистической регрессии}
	\label{ch4:fig:irls_qpfs_2d}
\end{figure}

На Рис.~\ref{ch4:fig:active_params_wrt_iters} показаны наборы активных параметров на итерациях для набора данных Boston Housing и нейронной сети с двумя скрытыми нейронами. 
Темные ячейки соответствуют активным параметрам, которые мы оптимизируем.
 
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figs/ch4/active_params_wrt_iters}	
	\caption{Множество активных параметров на протяжении оптимизационного процесса}
	\label{ch4:fig:active_params_wrt_iters}
\end{figure}

В рассмотренных примерах число обусловленности~$\kappa(\bH)$ для метода Ньютона на некоторых итерациях было чрезвычайно большим. 
Выбор активных параметров позволил значительно сократить число обусловленности. 

Приведём сравнение предложенного метода с существующими методами, а именно градиентным спуском~(GD), моментом Нестерова~\cite{nesterov1983momentum}, Adam~\cite{kingma2014adam} и оригинальным методом Ньютона. 
Проведены эксперименты для моделей нелинейной и логистической регрессий. 
Наборы данных были выбраны из репозитория UCI~\cite{uci2017}. 
Результаты показаны в таблицах~\ref{ch4:tbl:nonlin_reg_results} и \ref{ch4:tbl:log_reg_results}. 
Для каждого набора данных две строки таблиц содержат ошибки для тренировочной~(первая строка) и тестовой~(вторая строка) выборок. 
В таблице~\ref{ch4:tbl:nonlin_reg_results} приведена квадратичная ошибка, в таблице~\ref{ch4:tbl:log_reg_results}~--- кросс-энтропия.
Чтобы найти среднюю ошибку и ее стандартное отклонение использовалась процедура кросс валидации с разбиением на 5 фолдов. 
Предложенный метод показывает меньшую ошибку на трех из четырех наборов данных для нелинейной регрессии и на двух из трех наборов данных для логистической регрессии.

\begin{table}[h]
\footnotesize
	\caption{Средняя квадратичная ошибка рассматриваемых алгоритмов оптимизации для модели нелинейной регрессии}
	\label{ch4:tbl:nonlin_reg_results}
	\centering
	\begin{tabular}{|l|c|c|c|c|c|c|}
		\hline
		Выборка & \begin{tabular}[c]{@{}c@{}}\ $m$ \\ \ $n$ \end{tabular} 
		& GD 
		& Нестеров 
		& ADAM 
		& Ньютон 
		&
		\begin{tabular}[c]{@{}c@{}}QPFS+Ньютон\\ \end{tabular} \\ 
		\hline
		Boston House
		& 506
		& $27.2\pm4.6$
		& $46.0\pm11.0$
		& $35.4\pm2.5$           
		& $22.1\pm15.2$            
		& $20.9\pm10.4$   \\  
		Prices
		&\multicolumn{1}{c|}{13}
		& \multicolumn{1}{c|}{$32.4\pm5.6$}
		& \multicolumn{1}{c|}{$53.3\pm11.5$}
		& \multicolumn{1}{c|}{$37.8\pm7.0$}
		& \multicolumn{1}{c|}{$28.9\pm13.6$}
		& \multicolumn{1}{c|}{$\mathbf{24.5\pm9.4}$}\\ 
		\hline
		Communities
		& 1994
		& $48.0\pm6.4$
		& $31.4\pm2.8$
		& $23.3\pm3.7$        
		& $18.3\pm3.4$          
		&  $26.7\pm3.1$  \\ 
		and Crime
		&\multicolumn{1}{c|}{99}
		& \multicolumn{1}{c|}{$47,5\pm6.5$}
		& \multicolumn{1}{c|}{$32.9\pm4.3$}
		& \multicolumn{1}{c|}{$28,1\pm4.5$}
		& \multicolumn{1}{c|}{$28.8\pm3.6$}
		& \multicolumn{1}{c|}{$\mathbf{28.4\pm3.0}$} \\ 
		\hline
		Forest
		& 517
		& $18.9\pm0.4$
		& $1.83\pm0.4$
		& $1.81\pm0.6$             
		& $17.7\pm0.4$             
		& $17.9\pm0.4$   \\ 
		Fires
		&\multicolumn{1}{c|}{10}
		& \multicolumn{1}{c|}{$\mathbf{20.0\pm2.1}$}
		& \multicolumn{1}{c|}{ $20.2\pm2.2$}
		& \multicolumn{1}{c|}{ $\mathbf{20.0\pm2.0}$}
		& \multicolumn{1}{c|}{ $20.6\pm1.4$}
		& \multicolumn{1}{c|}{ $20.2\pm2.2$} \\ 
		\hline
		Residential
		& 372
		&  $51.6\pm17.7$
		&  $32.6\pm19.5$
		&  $30.0\pm24.8$            
		&  $35.5\pm24.7$            
		&   $30.3\pm10.7$ \\ 
		Building
		&\multicolumn{1}{c|}{103}
		& \multicolumn{1}{c|}{ $53.7\pm13.9$}
		& \multicolumn{1}{c|}{ $34.1\pm13.6$}
		& \multicolumn{1}{c|}{ $34.1\pm19.4$}
		& \multicolumn{1}{c|}{ $35.0\pm15.6$}
		& \multicolumn{1}{c|}{ $\mathbf{30.9\pm5.3}$} \\ 
		\hline
	\end{tabular}
\end{table}

\begin{table}[h]
\footnotesize
	\caption{Среднее значение кросс-энтропии рассматриваемых алгоритмов оптимизации для модели логистической регрессии}
	\label{ch4:tbl:log_reg_results}
	\centering
	\begin{tabular}{|l|c|c|c|c|c|c|}
		\hline
		Выборка & \begin{tabular}[c]{@{}c@{}}\ $m$ \\ \ $n$ \end{tabular} 
		& GD 
		& Нестеров 
		& ADAM 
		& Ньютон 
		&
		\begin{tabular}[c]{@{}c@{}}QPFS+Ньютон\\ \end{tabular} \\ 
		\hline
		Breast
		& 569
		& $0.6\pm0.1$
		& $0.4\pm0.1$
		& $0.8\pm0.2$           
		& $0.3\pm0.1$            
		& $0.2\pm0.1$   \\  
		Cancer
		&\multicolumn{1}{c|}{30}
		& \multicolumn{1}{c|}{$\mathbf{0.9\pm0.2}$}
		& \multicolumn{1}{c|}{$1.0\pm0.7$}
		& \multicolumn{1}{c|}{$1.2\pm0.2$}
		& \multicolumn{1}{c|}{$1.0\pm0.2$}
		& \multicolumn{1}{c|}{$1.1\pm0.3$}\\ 
		\hline
		Cardiotocography
		& 2126
		& $11.5\pm4.7$
		& $11.5\pm4.7$
		& $8.8\pm4.4$        
		& $11.5\pm5.7$          
		&  $7.7\pm4.2$  \\
		
		&\multicolumn{1}{c|}{21}
		& \multicolumn{1}{c|}{$11.6\pm5.8$}
		& \multicolumn{1}{c|}{$11.5\pm5.7$}
		& \multicolumn{1}{c|}{$9.0\pm2.6$}
		& \multicolumn{1}{c|}{$11.5\pm4.7$}
		& \multicolumn{1}{c|}{$\mathbf{7.7\pm4.7}$} \\ 
		\hline
		Climate Model
		& 540
		& $1.2\pm0.1$
		& $1.0\pm0.2$
		& $1.5\pm0.2$             
		& $1.0\pm0.5$             
		& $0.8\pm0.3$   \\ 
		Simulation Crashes
		&\multicolumn{1}{c|}{18}
		& \multicolumn{1}{c|}{$1.4\pm2.0$}
		& \multicolumn{1}{c|}{ $1.3\pm0.7$}
		& \multicolumn{1}{c|}{ $1.8\pm0.3$}
		& \multicolumn{1}{c|}{ $1.2\pm0.5$}
		& \multicolumn{1}{c|}{ $\mathbf{1.1\pm0.4}$} \\ 
		\hline
	\end{tabular}
\end{table}

